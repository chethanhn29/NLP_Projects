{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T11:02:25.962273Z","iopub.execute_input":"2023-07-03T11:02:25.962621Z","iopub.status.idle":"2023-07-03T11:02:25.980223Z","shell.execute_reply.started":"2023-07-03T11:02:25.962576Z","shell.execute_reply":"2023-07-03T11:02:25.979291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:25.996784Z","iopub.execute_input":"2023-07-03T11:02:25.997075Z","iopub.status.idle":"2023-07-03T11:02:27.320904Z","shell.execute_reply.started":"2023-07-03T11:02:25.997051Z","shell.execute_reply":"2023-07-03T11:02:27.319821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",sep=\",\",encoding=\"ISO-8859-1\")\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:27.326562Z","iopub.execute_input":"2023-07-03T11:02:27.329054Z","iopub.status.idle":"2023-07-03T11:02:27.391594Z","shell.execute_reply.started":"2023-07-03T11:02:27.329016Z","shell.execute_reply":"2023-07-03T11:02:27.390595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## First we will remove unnecessary Columns\ndf.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:27.396124Z","iopub.execute_input":"2023-07-03T11:02:27.398557Z","iopub.status.idle":"2023-07-03T11:02:27.409449Z","shell.execute_reply.started":"2023-07-03T11:02:27.398521Z","shell.execute_reply":"2023-07-03T11:02:27.408545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Rename the Columns \ndf.rename({\"v1\":\"label\",\"v2\":\"message\"},axis=1,inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:27.416000Z","iopub.execute_input":"2023-07-03T11:02:27.418703Z","iopub.status.idle":"2023-07-03T11:02:27.437189Z","shell.execute_reply.started":"2023-07-03T11:02:27.418669Z","shell.execute_reply":"2023-07-03T11:02:27.436294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To check the the dataset is Balanced or Imbalaced \ndf[\"label\"].value_counts(),sns.countplot(x=df[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:27.441421Z","iopub.execute_input":"2023-07-03T11:02:27.443838Z","iopub.status.idle":"2023-07-03T11:02:27.744477Z","shell.execute_reply.started":"2023-07-03T11:02:27.443803Z","shell.execute_reply":"2023-07-03T11:02:27.743774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see that dataset is imbalanced and out of 5572 SMS,we have\n1. 4825 ham SMS\n2. 747 Spam SMS","metadata":{}},{"cell_type":"code","source":"## Import nltk library to do NLP tasks \n# (*) Will import all modules from that library\nimport nltk\n## For stopwords\nfrom nltk.corpus import *\n# For Stemming and Lemmatization Preprocessing Models\nfrom nltk.stem import *\n## For regex Modlue to find patterns and preprorcessing of text\nimport re\n## Install wordnet \n!pip install wordnet\n## Import wordnet for Lemmatization process to find meaningful word\nnltk.download('wordnet')\n## To unzip the corpora wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:27.748744Z","iopub.execute_input":"2023-07-03T11:02:27.750935Z","iopub.status.idle":"2023-07-03T11:02:45.017930Z","shell.execute_reply.started":"2023-07-03T11:02:27.750900Z","shell.execute_reply":"2023-07-03T11:02:45.016670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Function for stemming and Lemmatization Text Preprocessing\n","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, use_stemming=True, use_lemmatization=True):\n    '''It Takes 3 Argumetns , first one is input(text) , input has to be in entries in dataframe,\n    if For Stemmed text then we have to put use_stemming as True and use_lemmatization as False,\n    and for Lemmatized text vice Versa.'''\n#each time the function is called, it initializes a fresh corpus list and processes \n#the text accordingly, ensuring that you get the desired stemmed or lemmatized corpus based on the arguments provided.\n    corpus = []\n    ## When tuning the parameters for better accuracy ,\n    ##we can change the stemmer/Lemmatizer to see variation of accuracy\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    \n    for i in range(len(text)):\n        review = re.sub('[^a-zA-Z]', ' ', text[i])\n        review = review.lower().split()\n        \n        if use_stemming:\n            review = [stemmer.stem(word) for word in review if word not in stopwords.words(\"english\")]\n        elif use_lemmatization:\n            review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words(\"english\")]\n        \n        review = \" \".join(review)\n        corpus.append(review)\n    \n    return corpus","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:45.020120Z","iopub.execute_input":"2023-07-03T11:02:45.020521Z","iopub.status.idle":"2023-07-03T11:02:45.030165Z","shell.execute_reply.started":"2023-07-03T11:02:45.020483Z","shell.execute_reply":"2023-07-03T11:02:45.029065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Give input for the first argument of preprocess_text funtion\ntext=df['message']\n## Stemmed text after Stemming \nStemmed_text = preprocess_text(text, use_stemming=True, use_lemmatization=False)\n## Stemmed text after Lemmatization\nLemmatized_text = preprocess_text(text, use_stemming=False, use_lemmatization=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:02:45.031706Z","iopub.execute_input":"2023-07-03T11:02:45.032039Z","iopub.status.idle":"2023-07-03T11:03:09.750699Z","shell.execute_reply.started":"2023-07-03T11:02:45.032007Z","shell.execute_reply":"2023-07-03T11:03:09.749688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Totally we have 3 types of Texts Now\n1.  Raw Text\n2. Stemmed Text\n3. Lemmatized Text\n\nWhile Checking for Accuracy of Model , try different types of texts to get Different Results ","metadata":{}},{"cell_type":"code","source":"## We can see that differnt text for stemming and lemmatization\nfor i in range(2490,2495):\n    print(Stemmed_text[i])\n    print(Lemmatized_text[i])\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:09.752232Z","iopub.execute_input":"2023-07-03T11:03:09.752593Z","iopub.status.idle":"2023-07-03T11:03:09.758873Z","shell.execute_reply.started":"2023-07-03T11:03:09.752560Z","shell.execute_reply":"2023-07-03T11:03:09.757472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import *\n## Import Word2vec\nfrom gensim.models import Word2Vec\n\n\ndef Text_representation_models(text_corpus,use_Word2Vec=True) :\n    X= []  # Initialize an empty list to store text representations\n\n    if use_Word2Vec :\n        # Training the Word2Vec model\n        model = Word2Vec(sentences=text_corpus, min_count=1,vector_size=100,window=5,workers=3)\n        # The Number of Words in the vocabulary and number Dimesions for each Word vector\n        #model.wv.vectors.shape\n\n        # Iterate over each text entry in the corpus\n        for entry in text_corpus:\n            # Retrieve Word2Vec vectors for each word in the entry and filter out words not in the model's vocabulary\n            word_vectors = [model.wv[word] for word in entry.split() if word in model.wv]\n\n            # Check if any valid word vectors are found\n            if len(word_vectors) > 0:\n                # Calculate the mean vector of all word vectors\n                entry_vector = np.mean(word_vectors, axis=0)\n            else:\n                # Assign a zero vector if no valid word vectors are found\n                entry_vector = np.zeros(model.vector_size)\n\n            # Append the entry vector to the Word2Vec representations list\n            X.append(entry_vector)\n\n        # Convert the Word2Vec representations list into a NumPy array\n        X= np.array(X)\n\n    else:\n        raise ValueError(\"Invalid text representation flag.\")\n        \n    ##  Using these models returns the Sparsed Matrix of text \n    return X\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:09.764329Z","iopub.execute_input":"2023-07-03T11:03:09.765689Z","iopub.status.idle":"2023-07-03T11:03:09.954672Z","shell.execute_reply.started":"2023-07-03T11:03:09.765654Z","shell.execute_reply":"2023-07-03T11:03:09.953617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"vector_size: This parameter sets the dimensionality of the word vectors or embeddings. It determines the length of the feature vectors representing each word in the corpus. A higher value for size can capture more nuanced relationships between words, but it also increases the computational complexity.\n\nwindow: The window size determines the maximum distance between the target word and its surrounding context words. It specifies the number of words before and after the target word that the model considers as context. For example, a window of 5 means that the model looks at five words before and five words after the target word in the training samples.\n\nmin_count: This parameter sets the minimum frequency threshold for words to be included in the vocabulary. Words that occur less frequently than min_count are ignored and not considered in the training process. Setting a higher value for min_count can help filter out rare words, reducing noise in the data and improving the quality of the learned embeddings.\n\nworkers: The workers parameter determines the number of parallel processes to use for training the model. By utilizing multiple workers, the training process can be accelerated, especially for large corpora. The recommended value depends on your hardware capabilities. Setting it to a higher value can speed up training, but it may consume more system resources.","metadata":{}},{"cell_type":"code","source":"## while checking for accuracy we can change the type of stemming and type of text representation\ntext_corpus=Stemmed_text\nfeatures=Text_representation_models(text_corpus,use_Word2Vec=True)\nfeatures,features.shape,type(features)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:09.956064Z","iopub.execute_input":"2023-07-03T11:03:09.956712Z","iopub.status.idle":"2023-07-03T11:03:10.581717Z","shell.execute_reply.started":"2023-07-03T11:03:09.956673Z","shell.execute_reply":"2023-07-03T11:03:10.580763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Word2Vec representation shows a smaller number of samples (27) compared to the other representations. This is because Word2Vec is a context-based embedding model that learns word representations based on the context of words in the corpus. It requires a sufficient amount of training data to capture meaningful word embeddings.\n\nIn the given code, the Word2Vec model is trained on the preprocessed text corpus, and the resulting word vectors are extracted. The number of unique words in the vocabulary (5572) represents the vocabulary size. However, the number of samples (27) corresponds to the number of unique sentences or documents in the corpus that contain at least one word from the vocabulary.","metadata":{}},{"cell_type":"code","source":"## We are converting target column from text format to numerical representation using pd_get dummies method\ntarget=pd.get_dummies(data=df[\"label\"],drop_first=True)\n# Target Column\nprint(target,\"\\n\\n\",target.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:10.583305Z","iopub.execute_input":"2023-07-03T11:03:10.583678Z","iopub.status.idle":"2023-07-03T11:03:10.600940Z","shell.execute_reply.started":"2023-07-03T11:03:10.583644Z","shell.execute_reply":"2023-07-03T11:03:10.600043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a function to transform the text data and to return Train and test data","metadata":{}},{"cell_type":"code","source":"# import Necessary Libraries to build model and to check performance of Model\nfrom sklearn.pipeline import Pipeline\n# To import Logistic regression Models\nfrom sklearn.linear_model import *\n#for Metrics\nfrom sklearn.metrics import *\n## To train_test_split method\nfrom sklearn.model_selection import *\n# For Naive Bayes method\nfrom sklearn.naive_bayes import *\n# For Support vector Machine\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:10.604206Z","iopub.execute_input":"2023-07-03T11:03:10.604509Z","iopub.status.idle":"2023-07-03T11:03:10.625479Z","shell.execute_reply.started":"2023-07-03T11:03:10.604481Z","shell.execute_reply":"2023-07-03T11:03:10.624470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Defined Functions get Train and Test data ","metadata":{}},{"cell_type":"code","source":"## To check the Function is Working Correctly or Not \ntext=df[\"message\"]\n## Assigning Target Column to traget variable\ntarget=target\nX_train, X_test, y_train, y_test=fit_data(text,target,use_stemming=True,use_lemmatization=False,use_Word2Vec=True)\nX_train","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:10.637524Z","iopub.execute_input":"2023-07-03T11:03:10.637880Z","iopub.status.idle":"2023-07-03T11:03:34.195274Z","shell.execute_reply.started":"2023-07-03T11:03:10.637839Z","shell.execute_reply":"2023-07-03T11:03:34.194395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model,X_test,y_test):\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    f1_micro = f1_score(y_test, predictions, average='micro')\n    f1_macro = f1_score(y_test, predictions, average='macro')\n    f1_weighted = f1_score(y_test, predictions, average='weighted')\n    recall_micro = recall_score(y_test, predictions, average='micro')\n    recall_macro = recall_score(y_test, predictions, average='macro')\n    recall_weighted = recall_score(y_test, predictions, average='weighted')\n    precision_micro = precision_score(y_test, predictions, average='micro')\n    precision_macro = precision_score(y_test, predictions, average='macro')\n    precision_weighted = precision_score(y_test, predictions, average='weighted')\n    cm = confusion_matrix(y_test, predictions)\n    \n    return {\n        'accuracy': accuracy,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'recall_micro': recall_micro,\n        'recall_macro': recall_macro,\n        'recall_weighted': recall_weighted,\n        'precision_micro': precision_micro,\n        'precision_macro': precision_macro,\n        'precision_weighted': precision_weighted,\n        'confusion_matrix': cm\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:34.196626Z","iopub.execute_input":"2023-07-03T11:03:34.197187Z","iopub.status.idle":"2023-07-03T11:03:34.208538Z","shell.execute_reply.started":"2023-07-03T11:03:34.197151Z","shell.execute_reply":"2023-07-03T11:03:34.207559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## import necessary libraries for models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import RidgeClassifier, PassiveAggressiveClassifier\n\n\n## List of Classification Models to check performance of each model on this task \nmodels = [\n    LogisticRegression(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(),\n    #GaussianNB(),## It Needs Non Negative data\n    #MultinomialNB(),## It Needs Non Negative data\n    #ComplementNB(),## It Needs Non Negative data\n    BernoulliNB(),\n    KNeighborsClassifier(),\n    XGBClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    GaussianProcessClassifier(),\n    ExtraTreesClassifier(),\n    RidgeClassifier(),\n    PassiveAggressiveClassifier()\n]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:03:34.225774Z","iopub.execute_input":"2023-07-03T11:03:34.226045Z","iopub.status.idle":"2023-07-03T11:03:34.665269Z","shell.execute_reply.started":"2023-07-03T11:03:34.226014Z","shell.execute_reply":"2023-07-03T11:03:34.664319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models:\n    results = evaluate_model(model, X_test, y_test)\n    print(\"Model:\", model)\n    print(\"Accuracy:\", results['accuracy'])\n    print(\"F1 Score (Micro):\", results['f1_micro'])\n    print(\"F1 Score (Macro):\", results['f1_macro'])\n    print(\"F1 Score (Weighted):\", results['f1_weighted'])\n    print(\"Recall (Micro):\", results['recall_micro'])\n    print(\"Recall (Macro):\", results['recall_macro'])\n    print(\"Recall (Weighted):\", results['recall_weighted'])\n    print(\"Precision (Micro):\", results['precision_micro'])\n    print(\"Precision (Macro):\", results['precision_macro'])\n    print(\"Precision (Weighted):\", results['precision_weighted'])\n    print(\"Confusion Matrix:\\n\", results['confusion_matrix'])\n    print(\"F1 Score (Average):\", (results['f1_weighted']+results['f1_micro']+results['f1_macro'])/3)\n\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:30:51.960763Z","iopub.execute_input":"2023-07-03T11:30:51.961122Z","iopub.status.idle":"2023-07-03T11:30:54.236710Z","shell.execute_reply.started":"2023-07-03T11:30:51.961092Z","shell.execute_reply":"2023-07-03T11:30:54.235682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}