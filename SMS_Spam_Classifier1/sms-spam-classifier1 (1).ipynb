{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T11:54:03.372884Z","iopub.execute_input":"2023-07-03T11:54:03.373246Z","iopub.status.idle":"2023-07-03T11:54:03.390617Z","shell.execute_reply.started":"2023-07-03T11:54:03.373217Z","shell.execute_reply":"2023-07-03T11:54:03.389469Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:03.393340Z","iopub.execute_input":"2023-07-03T11:54:03.393773Z","iopub.status.idle":"2023-07-03T11:54:04.005055Z","shell.execute_reply.started":"2023-07-03T11:54:03.393739Z","shell.execute_reply":"2023-07-03T11:54:04.004041Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",sep=\",\",encoding=\"ISO-8859-1\")\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.008121Z","iopub.execute_input":"2023-07-03T11:54:04.008421Z","iopub.status.idle":"2023-07-03T11:54:04.055208Z","shell.execute_reply.started":"2023-07-03T11:54:04.008396Z","shell.execute_reply":"2023-07-03T11:54:04.054000Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        v1                                                 v2 Unnamed: 2  \\\n0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n1      ham                      Ok lar... Joking wif u oni...        NaN   \n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3      ham  U dun say so early hor... U c already then say...        NaN   \n4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n...    ...                                                ...        ...   \n5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n5571   ham                         Rofl. Its true to its name        NaN   \n\n     Unnamed: 3 Unnamed: 4  \n0           NaN        NaN  \n1           NaN        NaN  \n2           NaN        NaN  \n3           NaN        NaN  \n4           NaN        NaN  \n...         ...        ...  \n5567        NaN        NaN  \n5568        NaN        NaN  \n5569        NaN        NaN  \n5570        NaN        NaN  \n5571        NaN        NaN  \n\n[5572 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.057398Z","iopub.execute_input":"2023-07-03T11:54:04.057791Z","iopub.status.idle":"2023-07-03T11:54:04.065758Z","shell.execute_reply.started":"2023-07-03T11:54:04.057759Z","shell.execute_reply":"2023-07-03T11:54:04.064759Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"## First we will remove unnecessary Columns\ndf.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.069362Z","iopub.execute_input":"2023-07-03T11:54:04.070075Z","iopub.status.idle":"2023-07-03T11:54:04.076537Z","shell.execute_reply.started":"2023-07-03T11:54:04.070039Z","shell.execute_reply":"2023-07-03T11:54:04.075518Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"## Rename the Columns \ndf.rename({\"v1\":\"label\",\"v2\":\"message\"},axis=1,inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.078767Z","iopub.execute_input":"2023-07-03T11:54:04.079462Z","iopub.status.idle":"2023-07-03T11:54:04.097189Z","shell.execute_reply.started":"2023-07-03T11:54:04.079425Z","shell.execute_reply":"2023-07-03T11:54:04.095872Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     label                                            message\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n...    ...                                                ...\n5567  spam  This is the 2nd time we have tried 2 contact u...\n5568   ham              Will Ì_ b going to esplanade fr home?\n5569   ham  Pity, * was in mood for that. So...any other s...\n5570   ham  The guy did some bitching but I acted like i'd...\n5571   ham                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## To check the the dataset is Balanced or Imbalaced \ndf[\"label\"].value_counts(),sns.countplot(x=df[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.098628Z","iopub.execute_input":"2023-07-03T11:54:04.099277Z","iopub.status.idle":"2023-07-03T11:54:04.400453Z","shell.execute_reply.started":"2023-07-03T11:54:04.099238Z","shell.execute_reply":"2023-07-03T11:54:04.399468Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(ham     4825\n spam     747\n Name: label, dtype: int64,\n <Axes: xlabel='label', ylabel='count'>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"#### From the above plot we can see that dataset is imbalanced and out of 5572 SMS,we have \n\n#### 1. 4825 ham SMS \n#### 2. 747 Spam SMS ","metadata":{}},{"cell_type":"code","source":"## Import nltk library to do NLP tasks \n# (*) Will import all modules from that library\nimport nltk\n## For stopwords\nfrom nltk.corpus import *\n# For Stemming and Lemmatization Preprocessing Models\nfrom nltk.stem import *\n## For regex Modlue to find patterns and preprorcessing of text\nimport re\n## Install wordnet \n!pip install wordnet\n## Import wordnet for Lemmatization process to find meaningful word\nnltk.download('wordnet')\n## To unzip the corpora wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:54:04.402003Z","iopub.execute_input":"2023-07-03T11:54:04.402821Z","iopub.status.idle":"2023-07-03T11:58:40.282979Z","shell.execute_reply.started":"2023-07-03T11:54:04.402776Z","shell.execute_reply":"2023-07-03T11:58:40.281722Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wordnet in /opt/conda/lib/python3.10/site-packages (0.0.1b2)\nRequirement already satisfied: colorama==0.3.9 in /opt/conda/lib/python3.10/site-packages (from wordnet) (0.3.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\nreplace /usr/share/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Define Function for stemming and Lemmatization Text Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, use_stemming=True, use_lemmatization=True):\n    '''It Takes 3 Argumetns , first one is input(text) , input has to be in entries in dataframe,\n    if For Stemmed text then we have to put use_stemming as True and use_lemmatization as False,\n    and for Lemmatized text vice Versa.'''\n#each time the function is called, it initializes a fresh corpus list and processes \n#the text accordingly, ensuring that you get the desired stemmed or lemmatized corpus based on the arguments provided.\n    corpus = []\n    ## When tuning the parameters for better accuracy ,\n    ##we can change the stemmer/Lemmatizer to see variation of accuracy\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    \n    for i in range(len(text)):\n        review = re.sub('[^a-zA-Z]', ' ', text[i])\n        review = review.lower().split()\n        \n        if use_stemming:\n            review = [stemmer.stem(word) for word in review if word not in stopwords.words(\"english\")]\n        elif use_lemmatization:\n            review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words(\"english\")]\n        \n        review = \" \".join(review)\n        corpus.append(review)\n    \n    return corpus\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:58:40.284767Z","iopub.execute_input":"2023-07-03T11:58:40.285532Z","iopub.status.idle":"2023-07-03T11:58:40.298037Z","shell.execute_reply.started":"2023-07-03T11:58:40.285492Z","shell.execute_reply":"2023-07-03T11:58:40.296916Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Check the Defined Function is Giving Lemmatized and stemmed Words Correctly or Not ","metadata":{}},{"cell_type":"code","source":"## Give input for the first argument of preprocess_text funtion\ntext=df['message']\n## Stemmed text after Stemming \nStemmed_text = preprocess_text(text, use_stemming=True, use_lemmatization=False)\n## Stemmed text after Lemmatization\nLemmatized_text = preprocess_text(text, use_stemming=False, use_lemmatization=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:58:40.299137Z","iopub.execute_input":"2023-07-03T11:58:40.299482Z","iopub.status.idle":"2023-07-03T11:59:05.092413Z","shell.execute_reply.started":"2023-07-03T11:58:40.299448Z","shell.execute_reply":"2023-07-03T11:59:05.091412Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## We can see that differnt text for stemming and lemmatization\nfor i in range(2490,2495):\n    print(Stemmed_text[i])\n    print(Lemmatized_text[i])\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.097282Z","iopub.execute_input":"2023-07-03T11:59:05.097650Z","iopub.status.idle":"2023-07-03T11:59:05.104621Z","shell.execute_reply.started":"2023-07-03T11:59:05.097616Z","shell.execute_reply":"2023-07-03T11:59:05.103596Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"dun b sad dun thk abt alreadi concentr ur paper k\ndun b sad dun thk abt already concentrate ur paper k\n\ngreet consid excus\ngreeting consider excused\n\ndrama pl enough famili struggl hot sun strang place reason ego go invit actual necess go wait seriou reppurcuss\ndrama pls enough family struggling hot sun strange place reason ego going invited actually necessity go wait serious reppurcussions\n\nreleas anoth italian one today cosign option\nreleased another italian one today cosign option\n\nmu tri figur much money everyon ga alcohol jay tri figur weed budget\nmu try figure much money everyone gas alcohol jay trying figure weed budget\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Now we have Stemmed Corpus but we still didnt convert it into Numerical/Vector represation to feed into Model\n\n### Here We are going to Perform 2 types of Text representation of Independent features(Text data)\n1.  BOW Model\n2. TF-IDF Model\n\n#### For now we are going to give one type of Stemmed text either Stemmed or Lemmatized text, based on the accuracy of the model we are going to change the parametrs like \n1. Text Representation model(BOW or TF-IDF)\n2. Max features in Text representation model\n2. Type of Stemming\n","metadata":{}},{"cell_type":"markdown","source":"## Text representaion Models\n\nfor More about these models refer [feature extraction from text models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import *\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.106565Z","iopub.execute_input":"2023-07-03T11:59:05.106924Z","iopub.status.idle":"2023-07-03T11:59:05.117335Z","shell.execute_reply.started":"2023-07-03T11:59:05.106891Z","shell.execute_reply":"2023-07-03T11:59:05.116355Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def Text_representation_models(text_corpus,use_BOW=True,use_TFIDF=True,max_features=None) :\n    if use_BOW :\n        vectorizer = CountVectorizer(max_features=max_features) \n    elif use_TFIDF :\n        vectorizer =TfidfVectorizer(max_features=max_features)   \n    else:\n        raise ValueError(\"Invalid text representation flag.\")\n        \n    X = vectorizer.fit_transform(text_corpus).toarray()\n    ##  Using these models returns the Sparsed Matrix of text \n    return X","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.118849Z","iopub.execute_input":"2023-07-03T11:59:05.119487Z","iopub.status.idle":"2023-07-03T11:59:05.127390Z","shell.execute_reply.started":"2023-07-03T11:59:05.119450Z","shell.execute_reply":"2023-07-03T11:59:05.126396Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"## while checking for accuracy we can change the type of stemming and type of text representation\ntext_corpus=Stemmed_text\nfeatures=Text_representation_models(text_corpus,use_BOW=True,use_TFIDF=False,max_features=5000)\nfeatures,features.shape,type(features)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.129003Z","iopub.execute_input":"2023-07-03T11:59:05.129664Z","iopub.status.idle":"2023-07-03T11:59:05.349324Z","shell.execute_reply.started":"2023-07-03T11:59:05.129621Z","shell.execute_reply":"2023-07-03T11:59:05.348242Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]),\n (5572, 5000),\n numpy.ndarray)"},"metadata":{}}]},{"cell_type":"markdown","source":"### we got features now but we still have target column which is still in text format we can convert that using one_hot or Label Encoding or pd.get_dummies method","metadata":{}},{"cell_type":"code","source":"## We are converting target column from text format to numerical representation using pd_get dummies method\ntarget=pd.get_dummies(data=df[\"label\"],drop_first=True)\n# Target Column\nprint(target,\"\\n\\n\",target.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.351498Z","iopub.execute_input":"2023-07-03T11:59:05.352070Z","iopub.status.idle":"2023-07-03T11:59:05.366968Z","shell.execute_reply.started":"2023-07-03T11:59:05.352018Z","shell.execute_reply":"2023-07-03T11:59:05.365920Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"      spam\n0        0\n1        0\n2        1\n3        0\n4        0\n...    ...\n5567     1\n5568     0\n5569     0\n5570     0\n5571     0\n\n[5572 rows x 1 columns] \n\n spam\n0       4825\n1        747\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Here spam is Converted as 1 and ham is Converted as 0 , we can check for both spam and ham target column by deleting other column,Based on the accuracy of final model.","metadata":{}},{"cell_type":"code","source":"# import Necessary Libraries to build model and to check performance of Model\nfrom sklearn.pipeline import Pipeline\n# To import Logistic regression Models\nfrom sklearn.linear_model import *\n#for Metrics\nfrom sklearn.metrics import *\n## To train_test_split method\nfrom sklearn.model_selection import *\n# For Naive Bayes method\nfrom sklearn.naive_bayes import *\n# For Support vector Machine\nfrom sklearn.svm import SVC\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.369305Z","iopub.execute_input":"2023-07-03T11:59:05.369630Z","iopub.status.idle":"2023-07-03T11:59:05.379172Z","shell.execute_reply.started":"2023-07-03T11:59:05.369600Z","shell.execute_reply":"2023-07-03T11:59:05.377985Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Define a function to transform the text data and to return Train and test  data ","metadata":{}},{"cell_type":"code","source":"def fit_data(text,target,use_stemming=True,use_lemmatization=True,use_BOW=True,use_TFIDF=True):\n    # Code to evaluate the model using the provided data and calculate performance metrics\n    ''' if we use stemming we have 2 representaions for that BOW and TFIDF,same is for Lemmatization\n     and this function takes 5 arguments and 1 st one is the type model,\n     for 2nd and 3rd argument we can give only Preprocessing step as True\n     for 4th and 5th argument ,we can give only one type of Text representation as True '''\n    # Initialize the features variable\n    features = np.array([])    ## Text Preprocessing and preparing the features \n    stemmed_text=preprocess_text(text, use_stemming=True, use_lemmatization=False)\n    lemmatized_text=preprocess_text(text, use_stemming=False, use_lemmatization=True)\n    if use_stemming and  use_BOW :\n        text_corpus=stemmed_text\n        features=Text_representation_models(text_corpus,use_BOW=True,use_TFIDF=False,max_features=5000)\n    \n    elif use_stemming and  use_TFIDF :\n        text_corpus=stemmed_text\n        features=Text_representation_models(text_corpus,use_BOW=False,use_TFIDF=True,max_features=5000) \n        \n    elif use_lemmatization and use_BOW :  \n        text_corpus=lemmatized_text\n        features=Text_representation_models(text_corpus,use_BOW=True,use_TFIDF=False,max_features=5000)\n    \n    elif use_lemmatization and use_TFIDF:\n            text_corpus=lemmatized_text\n            features=Text_representation_models(text_corpus,use_BOW=False,use_TFIDF=True,max_features=5000)\n\n    else :\n        raise ValueError(\"Invalid combination of text preprocessing and text representation  flags.\")\n    \n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(features,target,test_size=0.3,random_state=50)\n\n    return X_train, X_test, y_train, y_test\n        ","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.381568Z","iopub.execute_input":"2023-07-03T11:59:05.381929Z","iopub.status.idle":"2023-07-03T11:59:05.394306Z","shell.execute_reply.started":"2023-07-03T11:59:05.381898Z","shell.execute_reply":"2023-07-03T11:59:05.393299Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"text=df[\"message\"]\n## Assigning Target Column to traget variable\ntarget=target\nX_train, X_test, y_train, y_test=fit_data(text,target,use_stemming=True,use_lemmatization=False,use_BOW=True,use_TFIDF=False)\nX_train","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:05.396465Z","iopub.execute_input":"2023-07-03T11:59:05.396880Z","iopub.status.idle":"2023-07-03T11:59:28.037753Z","shell.execute_reply.started":"2023-07-03T11:59:05.396849Z","shell.execute_reply":"2023-07-03T11:59:28.036753Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Define a function to  evaluate the model results ","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model,X_test,y_test):\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    f1_micro = f1_score(y_test, predictions, average='micro')\n    f1_macro = f1_score(y_test, predictions, average='macro')\n    f1_weighted = f1_score(y_test, predictions, average='weighted')\n    recall_micro = recall_score(y_test, predictions, average='micro')\n    recall_macro = recall_score(y_test, predictions, average='macro')\n    recall_weighted = recall_score(y_test, predictions, average='weighted')\n    precision_micro = precision_score(y_test, predictions, average='micro')\n    precision_macro = precision_score(y_test, predictions, average='macro')\n    precision_weighted = precision_score(y_test, predictions, average='weighted')\n    cm = confusion_matrix(y_test, predictions)\n    \n    return {\n        'accuracy': accuracy,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'recall_micro': recall_micro,\n        'recall_macro': recall_macro,\n        'recall_weighted': recall_weighted,\n        'precision_micro': precision_micro,\n        'precision_macro': precision_macro,\n        'precision_weighted': precision_weighted,\n        'confusion_matrix': cm\n    }","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:28.046203Z","iopub.execute_input":"2023-07-03T11:59:28.046942Z","iopub.status.idle":"2023-07-03T11:59:28.074312Z","shell.execute_reply.started":"2023-07-03T11:59:28.046905Z","shell.execute_reply":"2023-07-03T11:59:28.073014Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Define a function to get Combination of Preprocessed data and Text representation and to split the data and iterate over models using other Defined Functions","metadata":{}},{"cell_type":"code","source":"def find_best_model(models,text,target):\n    best_model = None\n    best_f1_score = float('-inf')\n\n    for use_stemming in [True, False]:\n        for use_lemmatization in [True, False]:\n            for use_BOW in [True, False]:\n                for use_TFIDF in [True, False]:\n                    # Ensure only one type of preprocessing is used\n                    if ((use_stemming and use_lemmatization) or (not use_stemming and not use_lemmatization)) :\n                        continue #skip if both stemming and lemmatization enabled and if  both are disabled\n            \n                    if ((use_BOW and use_TFIDF) or (not use_BOW and not use_TFIDF)):\n                        continue  # Skip if both BOW and TFIDF are enabled or if both are  disabled\n                    try:\n                        ## Get Train and test data by defined function\n                        X_train, X_test, y_train, y_test = fit_data(text, target, use_stemming, use_lemmatization, use_BOW, use_TFIDF)\n                    except ValueError:\n                        continue  # Skip the invalid combination\n                        '''Now we have train and test data now we can fit the model \n                                with train data and iterate over all models'''\n                    ## Train the model with training data\n                    for model in models:\n                        model.fit(X_train, y_train)\n                        # Get Performance of Model for Test data\n                        model_results = evaluate_model(model, X_test, y_test)\n                        f1_avg = (model_results['f1_micro'] + model_results['f1_macro'] + model_results['f1_weighted']) / 3\n\n                        # Print the results for each model and parameter combination\n                        print(\"Model:\", model)\n                        print(\"Use Stemming:\", use_stemming)\n                        print(\"Use Lemmatization:\", use_lemmatization)\n                        print(\"Use BOW:\", use_BOW)\n                        print(\"Use TFIDF:\", use_TFIDF)\n                        print(\"Accuracy:\", model_results['accuracy'])\n                        print(\"F1 Score (Micro):\", model_results['f1_micro'])\n                        print(\"F1 Score (Macro):\", model_results['f1_macro'])\n                        print(\"F1 Score (Weighted):\", model_results['f1_weighted'])\n                        print(\"Recall (Micro):\", model_results['recall_micro'])\n                        print(\"Recall (Macro):\", model_results['recall_macro'])\n                        print(\"Recall (Weighted):\", model_results['recall_weighted'])\n                        print(\"Precision (Micro):\", model_results['precision_micro'])\n                        print(\"Precision (Macro):\", model_results['precision_macro'])\n                        print(\"Precision (Weighted):\", model_results['precision_weighted'])\n                        print(\"Confusion Matrix:\\n\", model_results['confusion_matrix'])\n                        print(\"F1 Score (Average):\" ,f1_avg)\n                        print()\n\n                        # Update the best model if necessary\n                        if f1_avg > best_f1_score:\n                            best_model = (model, use_stemming, use_lemmatization, use_BOW, use_TFIDF)\n                            best_f1_score = f1_avg\n\n    return best_model","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:28.075934Z","iopub.execute_input":"2023-07-03T11:59:28.076814Z","iopub.status.idle":"2023-07-03T11:59:28.120771Z","shell.execute_reply.started":"2023-07-03T11:59:28.076635Z","shell.execute_reply":"2023-07-03T11:59:28.119370Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"## import necessary libraries for models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import RidgeClassifier, PassiveAggressiveClassifier\n\n\n## List of Classification Models to check performance of each model on this task \nmodels = [\n    LogisticRegression(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(),\n    GaussianNB(),\n    MultinomialNB(),\n    ComplementNB(),\n    BernoulliNB(),\n    KNeighborsClassifier(),\n    XGBClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    GaussianProcessClassifier(),\n    ExtraTreesClassifier(),\n    RidgeClassifier(),\n    PassiveAggressiveClassifier()\n]\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:28.121844Z","iopub.execute_input":"2023-07-03T11:59:28.122178Z","iopub.status.idle":"2023-07-03T11:59:28.334801Z","shell.execute_reply.started":"2023-07-03T11:59:28.122147Z","shell.execute_reply":"2023-07-03T11:59:28.333765Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## To Check Performance of each models for different Combination of Text Preprocessing and representaion and different Models\n## Assign the text data it will convert into the features \n#text=df[\"message\"]\n## Assigning Target Column to traget variable\n#target=target\n#best_model = find_best_model(models,text,target)\n#print(\"Best Model:\", best_model)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T11:59:28.337006Z","iopub.execute_input":"2023-07-03T11:59:28.338202Z","iopub.status.idle":"2023-07-03T11:59:28.346651Z","shell.execute_reply.started":"2023-07-03T11:59:28.338163Z","shell.execute_reply":"2023-07-03T11:59:28.345763Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def find_best_model(models, text, target):\n    best_model = None\n    best_f1_score = float('-inf')\n    model_results = []  # Store the results of each model\n\n    for use_stemming in [True, False]:\n        for use_lemmatization in [True, False]:\n            for use_BOW in [True, False]:\n                for use_TFIDF in [True, False]:\n                    # Ensure only one type of preprocessing is used\n                    if ((use_stemming and use_lemmatization) or (not use_stemming and not use_lemmatization)):\n                        continue  # Skip if both stemming and lemmatization are enabled or if both are disabled\n\n                    if ((use_BOW and use_TFIDF) or (not use_BOW and not use_TFIDF)):\n                        continue  # Skip if both BOW and TFIDF are enabled or if both are disabled\n                    try:\n                        # Get Train and test data by defined function\n                        X_train, X_test, y_train, y_test = fit_data(text, target, use_stemming, use_lemmatization, use_BOW, use_TFIDF)\n                        y_train = np.array(y_train).ravel()  # Reshape y_train to a 1D array\n                        y_test = np.array(y_test).ravel()  # Reshape y_test to a 1D array\n                    except ValueError:\n                        continue  # Skip the invalid combination\n\n                    # Train the model with training data\n                    for model in models:\n                        model.fit(X_train, y_train)\n                        # Get Performance of Model for Test data\n                        model_results_dict = evaluate_model(model, X_test, y_test)\n                        f1_avg = (model_results_dict['f1_micro'] + model_results_dict['f1_macro'] + model_results_dict['f1_weighted']) / 3\n\n                        # Store the results in a dictionary\n                        results = {\n                            'Model': model,\n                            'Use Stemming': use_stemming,\n                            'Use Lemmatization': use_lemmatization,\n                            'Use BOW': use_BOW,\n                            'Use TFIDF': use_TFIDF,\n                            'Accuracy': model_results_dict['accuracy'],\n                            'F1 Score (Micro)': model_results_dict['f1_micro'],\n                            'F1 Score (Macro)': model_results_dict['f1_macro'],\n                            'F1 Score (Weighted)': model_results_dict['f1_weighted'],\n                            'Recall (Micro)': model_results_dict['recall_micro'],\n                            'Recall (Macro)': model_results_dict['recall_macro'],\n                            'Recall (Weighted)': model_results_dict['recall_weighted'],\n                            'Precision (Micro)': model_results_dict['precision_micro'],\n                            'Precision (Macro)': model_results_dict['precision_macro'],\n                            'Precision (Weighted)': model_results_dict['precision_weighted'],\n                            'Confusion Matrix': model_results_dict['confusion_matrix'],\n                            'F1 Score (Average)': f1_avg\n                        }\n                        model_results.append(results)  # Add the results to the list\n\n                        # Update the best model if necessary\n                        if f1_avg > best_f1_score:\n                            best_model = (model, use_stemming, use_lemmatization, use_BOW, use_TFIDF)\n                            best_f1_score = f1_avg\n\n    return best_model, model_results","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:07:55.973257Z","iopub.execute_input":"2023-07-03T12:07:55.973647Z","iopub.status.idle":"2023-07-03T12:07:55.987565Z","shell.execute_reply.started":"2023-07-03T12:07:55.973616Z","shell.execute_reply":"2023-07-03T12:07:55.986358Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"## To Check Performance of each models for different Combination of Text Preprocessing and representaion and different Models\n## Assign the text data it will convert into the features \ntext=df[\"message\"]\n## Assigning Target Column to traget variable\ntarget=target\n# Call the function and get the best model and model results\nbest_model, results = find_best_model(models, text, target)\n\n# Print the best model\nprint(\"Best Model:\", best_model)\n# Create a DataFrame from the results list\ndf_results = pd.DataFrame(model_results)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:08:11.303396Z","iopub.execute_input":"2023-07-03T12:08:11.303790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the all of the above Classification Models we can see that MLP Classifier Model  with Lemmatization as Preprocessing and TFIDF as Text Representation is Doing a better job for this task and also its Overall accuracy  Precision, Recall F1 scores are good compared to others.\n\n#### Try to Increase the Model accuracy and Avg F1 Score by \n1. changing Parameters of the Model and Trying  Different Models for Classification\n2. Changing Text Preprocessing techniques \n3. Changing Number of Maximum(Best) features of Text Represetation Model.\n4. Trying different Stemmers or Lemmatizers Models \n5. Changing the Categorical Encoding of the Target Column\n6. Checking if we Provide higher Values to Spam or Ham.[ 0 or 1]\n7. Changing the Dimensions of Train and test data\n8. Check  Performance by Adding Validation Data\n9. Changing the Random seed while splitting the data  \n10. Data Sampling for the Imbalanced data\n11. Cross Validation\n12. Regularization Techniques to prevent Overfitting\n13. Try Hyperparameter Tuning With Gridsearch, RandomSearch,Bayestian Optimazation\n14. Check for the other Text Representation models like word2vec, ngram\n15. Check for Different Combination of words while vectorizing the words in BOW or TFIDF in ngram range paramter and try to change other parameters of Text Representation Models you can get parameters set by model by [get_params](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_params)and  you can change it according to accuracy you can check features names or (Words that got Vectorized using Text representation Model [vectorizer.get_feature_names_out(input_features=None)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out)","metadata":{}}]}