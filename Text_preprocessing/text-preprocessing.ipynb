{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-31T12:41:14.051905Z","iopub.execute_input":"2023-08-31T12:41:14.052318Z","iopub.status.idle":"2023-08-31T12:41:14.091808Z","shell.execute_reply.started":"2023-08-31T12:41:14.052276Z","shell.execute_reply":"2023-08-31T12:41:14.090678Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",encoding='ISO-8859-1')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:41:14.093737Z","iopub.execute_input":"2023-08-31T12:41:14.094737Z","iopub.status.idle":"2023-08-31T12:41:14.152928Z","shell.execute_reply.started":"2023-08-31T12:41:14.094701Z","shell.execute_reply":"2023-08-31T12:41:14.151655Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"        v1                                                 v2 Unnamed: 2  \\\n0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n1      ham                      Ok lar... Joking wif u oni...        NaN   \n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3      ham  U dun say so early hor... U c already then say...        NaN   \n4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n...    ...                                                ...        ...   \n5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n5571   ham                         Rofl. Its true to its name        NaN   \n\n     Unnamed: 3 Unnamed: 4  \n0           NaN        NaN  \n1           NaN        NaN  \n2           NaN        NaN  \n3           NaN        NaN  \n4           NaN        NaN  \n...         ...        ...  \n5567        NaN        NaN  \n5568        NaN        NaN  \n5569        NaN        NaN  \n5570        NaN        NaN  \n5571        NaN        NaN  \n\n[5572 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=df[[\"v1\",\"v2\"]]\ndf.rename(columns={\"v1\":\"label\",\"v2\":\"message\"},inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:41:14.156719Z","iopub.execute_input":"2023-08-31T12:41:14.157720Z","iopub.status.idle":"2023-08-31T12:41:14.179472Z","shell.execute_reply.started":"2023-08-31T12:41:14.157672Z","shell.execute_reply":"2023-08-31T12:41:14.177931Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/317735033.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.rename(columns={\"v1\":\"label\",\"v2\":\"message\"},inplace=True)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     label                                            message\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n...    ...                                                ...\n5567  spam  This is the 2nd time we have tried 2 contact u...\n5568   ham              Will Ì_ b going to esplanade fr home?\n5569   ham  Pity, * was in mood for that. So...any other s...\n5570   ham  The guy did some bitching but I acted like i'd...\n5571   ham                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Lets see How data Looks Like ","metadata":{}},{"cell_type":"code","source":"for i in range(100,110):\n    print(f\"{df['message'][i]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:41:14.182512Z","iopub.execute_input":"2023-08-31T12:41:14.183064Z","iopub.status.idle":"2023-08-31T12:41:14.189314Z","shell.execute_reply.started":"2023-08-31T12:41:14.183021Z","shell.execute_reply":"2023-08-31T12:41:14.188149Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Okay name ur price as long as its legal! Wen can I pick them up? Y u ave x ams xx\n\nI'm still looking for a car to buy. And have not gone 4the driving test yet.\n\nAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n\nwow. You're right! I didn't mean to do that. I guess once i gave up on boston men and changed my search location to nyc, something changed. Cuz on my signin page it still says boston.\n\nUmma my life and vava umma love you lot dear\n\nThanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\n\nAight, I'll hit you up when I get some cash\n\nHow would my ip address test that considering my computer isn't a minecraft server\n\nI know! Grumpy old people. My mom was like you better not be lying. Then again I am always the one to play jokes...\n\nDont worry. I guess he's busy.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## We are getting text data , so we cant feed this data into final model for classification of is it a Spam mail or not.\n### We have to follow these steps for final model.\n- Perform  All kind of Text Preprocessing techniques to get Cleaned data, Because we cant provide the These data directly into Text2Vec Model, Which doesnt give proper Vector representation.\n- Combine this data along with \"label\" and divide them into train and testing data, Normal ration is 70:30, we can also include Validation data.\n- Then Build Different Classifiers and then Calculate accuracy,AUC Cuurve,ROC Curve,F1 Score,Precision,Recall and other Relevant Metrics.\n- Then Plot the Train and test data loss and Accuracy plot.\n- Compare with Different Models Accuracy and Loss,then Select the Best Model.\n- Hypertune the model using Grid seach,Random search.\n- Now Model is Ready.","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing\n-  Removing Punctuations.\n-  Removing Non-Alphabetic Characters(Removing Digits and other characters like /\\|\";.,)\n-  Removing Whitespaces.\n-  Lowercasing the text(the Text2vec Model treat it as differnt characters , if it's not converted).\n-  Tokenization(Word,Sentence)\n-  Stemming or Lemmatization\n-  Removing Stopwords.\n-  Joining all Words into Each sentence and Create Text_Corpus to Convert Text2Vec Using text2vec model.","metadata":{}},{"cell_type":"code","source":"## Import nltk library to do NLP tasks \n# (*) Will import all modules from that library\nimport nltk\n## For stopwords\nfrom nltk.corpus import *\n# For Stemming and Lemmatization Preprocessing Models\nfrom nltk.stem import *\n## For regex Modlue to find patterns and preprorcessing of text\nimport re\n## Install wordnet \n!pip install wordnet\n## Import wordnet for Lemmatization process to find meaningful word\nnltk.download('wordnet')\n## To unzip the corpora wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n!pip install gensim\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:41:14.190812Z","iopub.execute_input":"2023-08-31T12:41:14.192086Z","iopub.status.idle":"2023-08-31T12:41:44.235386Z","shell.execute_reply.started":"2023-08-31T12:41:14.192042Z","shell.execute_reply":"2023-08-31T12:41:44.234242Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Collecting wordnet\n  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting colorama==0.3.9 (from wordnet)\n  Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\nBuilding wheels for collected packages: wordnet\n  Building wheel for wordnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10520 sha256=bf38b259a2ca403e520e84f02a308d1e659bc9a1956b077a813357a98eaa4cbe\n  Stored in directory: /root/.cache/pip/wheels/c0/a1/e8/4649c8712033dcdbd1e64a0fc75216a5d1769665852c36b4f9\nSuccessfully built wordnet\nInstalling collected packages: colorama, wordnet\n  Attempting uninstall: colorama\n    Found existing installation: colorama 0.4.6\n    Uninstalling colorama-0.4.6:\n      Successfully uninstalled colorama-0.4.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbayesian-optimization 1.4.3 requires colorama>=0.4.6, but you have colorama 0.3.9 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed colorama-0.3.9 wordnet-0.0.1b2\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.1)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## For Text Preprocessing  with Stemming \n- Removing Non-alphabetic Characters\n- lowercase all the Words\n- Split all the words in the senteces to individual words\n- Applyig Stemming for each word after checking if the word is present in the stpowords list or not\n- joining all the Words for Single Sentence\n- Storing each sentence in the list in each index position of list","metadata":{}},{"cell_type":"markdown","source":"## For Stemming\n- Gives root word or each word , it may have meaning or not in the real world.","metadata":{}},{"cell_type":"code","source":"stemmer=PorterStemmer()\ncorpus=[]\nfor i in range(len(df['message'])):\n    review=re.sub('[^a-zA-Z]',' ',df['message'][i]) ## First remove the non word characters\n    review=review.lower() #Lowercasing the text\n    ## Word Tokenizing\n    review=review.split() ## To Converting sentences into words\n    ## Stemming Process\n    review=[stemmer.stem(word) for word in review if not word in stopwords.words(\"english\")]\n    ## Joining each word in a sentence\n    review=\" \".join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:41:44.237296Z","iopub.execute_input":"2023-08-31T12:41:44.237631Z","iopub.status.idle":"2023-08-31T12:41:56.776921Z","shell.execute_reply.started":"2023-08-31T12:41:44.237597Z","shell.execute_reply":"2023-08-31T12:41:56.775500Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## To check Whether words of each sentences are stemmed or Not\nprint(f\"Raw Text  before  preprocessing of text and Stemming  is applied\\n{df['message'][1]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Stemming  is applied \\n {corpus[1]}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:44:37.163271Z","iopub.execute_input":"2023-08-31T12:44:37.163650Z","iopub.status.idle":"2023-08-31T12:44:37.169619Z","shell.execute_reply.started":"2023-08-31T12:44:37.163621Z","shell.execute_reply":"2023-08-31T12:44:37.168403Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The Raw Text  before Stemming amd preprocessing of text is applied\nOk lar... Joking wif u oni...\n\n\nThe Sentence after Stemming amd preprocessing of text is applied \nok lar joke wif u oni\n","output_type":"stream"}]},{"cell_type":"markdown","source":"we can see that the symbols are preprocessed and also words are getting stemmed","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing of text with Lemmatization\n- Lemmatizationn gives the Root words of words which has meaning in the real world.\n- For this task only one thing has to change from the previous code , which is changing stemming into lemmatization\n\n - Changes \n    - change the stemmer of stemming into Lemmatizing stemmer which is WordNetLemmatizer(there are others , u can choose whichever giving good results)\n    - Changing the Stemming process to Lemmatization\n","metadata":{}},{"cell_type":"code","source":"stemmer=WordNetLemmatizer()\ncorpus=[]\nfor i in range(len(df['message'])):\n    review=re.sub('[^a-zA-Z]',' ',df['message'][i]) ## First remove the non word characters\n    review=review.lower() #Lowercasing the text\n    ## Word Tokenizing\n    review=review.split() ## To Converting sentences into words\n    #Lemmatization process\n    review=[stemmer.lemmatize(word) for word in review if not word in stopwords.words(\"english\")]\n    review=\" \".join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:52:50.811139Z","iopub.execute_input":"2023-08-31T12:52:50.811572Z","iopub.status.idle":"2023-08-31T12:53:03.681313Z","shell.execute_reply.started":"2023-08-31T12:52:50.811542Z","shell.execute_reply":"2023-08-31T12:53:03.680149Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## To check Whether words of each sentences are stemmed or Not\nprint(f\"Raw Text  before  preprocessing of text and Lemmatization  is applied\\n{df['message'][5]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Lemmatization  is applied \\n {corpus[5]}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:57:12.603055Z","iopub.execute_input":"2023-08-31T12:57:12.603469Z","iopub.status.idle":"2023-08-31T12:57:12.610434Z","shell.execute_reply.started":"2023-08-31T12:57:12.603438Z","shell.execute_reply":"2023-08-31T12:57:12.609186Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Raw Text  before  preprocessing of text and Lemmatization  is applied\nFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n\n\nText after preprocessing of text and Lemmatization  is applied \n freemsg hey darling week word back like fun still tb ok xxx std chgs send rcv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Since there is only one chaange in  getting Lemmatized and Stemmed text , we can define funtion and also instead of usign split funtion to create indivual words in a sentece, we can use tokenize funtion from nltk word tokenize","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, use_stemming=True, use_lemmatization=False):\n    corpus = []\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words(\"english\"))\n    \n    for i in range(len(text)):\n        # Remove non-alphanumeric characters and convert to lowercase\n        review = re.sub('[^a-zA-Z]', ' ', text[i]).lower()\n        \n        # Tokenize the text\n        words = nltk.word_tokenize(review)\n        # we can also use this words=review.split()\n        # we can use  gensim.utils.simple_preprocess() funtion\n        \n        # Apply stemming or lemmatization and remove stopwords\n        if use_stemming:\n            words = [stemmer.stem(word) for word in words if word not in stop_words]\n        elif use_lemmatization:\n            words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n        \n        # Join the processed words back into a sentence\n        review = \" \".join(words)\n        corpus.append(review)\n    \n    return corpus","metadata":{"execution":{"iopub.status.busy":"2023-08-31T12:59:22.647261Z","iopub.execute_input":"2023-08-31T12:59:22.647757Z","iopub.status.idle":"2023-08-31T12:59:22.656614Z","shell.execute_reply.started":"2023-08-31T12:59:22.647699Z","shell.execute_reply":"2023-08-31T12:59:22.655135Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Call the funtion to get each corpus\nStemmed_corpus=preprocess_text(text=df[\"message\"], use_stemming=True, use_lemmatization=False)\nLemmatized_corpus=preprocess_text(text=df[\"message\"], use_stemming=False, use_lemmatization=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:02:16.649905Z","iopub.execute_input":"2023-08-31T13:02:16.650338Z","iopub.status.idle":"2023-08-31T13:02:19.960214Z","shell.execute_reply.started":"2023-08-31T13:02:16.650306Z","shell.execute_reply":"2023-08-31T13:02:19.959022Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Raw Text  before  preprocessing of text and Lemmatization  is applied\nFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n\n\nText after preprocessing of text and Stemming  is applied \n freemsg hey darl week word back like fun still tb ok xxx std chg send rcv\n\n\nText after preprocessing of text and Lemmatization  is applied \n freemsg hey darling week word back like fun still tb ok xxx std chgs send rcv\n","output_type":"stream"}]},{"cell_type":"code","source":"## To compare raw_corpus,Stemmed_corpus,Lemmatized corpus\nprint(f\"Raw Text  before  preprocessing of text and Lemmatization  is applied\\n{df['message'][10]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Stemming  is applied \\n {Stemmed_corpus[10]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Lemmatization  is applied \\n {Lemmatized_corpus[10]}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:04:42.148188Z","iopub.execute_input":"2023-08-31T13:04:42.148618Z","iopub.status.idle":"2023-08-31T13:04:42.155205Z","shell.execute_reply.started":"2023-08-31T13:04:42.148585Z","shell.execute_reply":"2023-08-31T13:04:42.153614Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Raw Text  before  preprocessing of text and Lemmatization  is applied\nI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n\n\nText after preprocessing of text and Stemming  is applied \n gon na home soon want talk stuff anymor tonight k cri enough today\n\n\nText after preprocessing of text and Lemmatization  is applied \n gon na home soon want talk stuff anymore tonight k cried enough today\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### See in the Stemming Process the Word cried is converted as cri,but in Lemmatization which gives as cried which makes a meaning in the word","metadata":{}},{"cell_type":"markdown","source":"# Tokenization\nTokenization in NLP is the process by which a large quantity of text is divided into smaller parts called tokens.\n- [NLTK Tokenization Docs](https://www.nltk.org/api/nltk.tokenize.html) \n\n## Word Tokenization ","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ntext = \"God is Great! I won a lottery.\"\nprint(word_tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:09:20.345173Z","iopub.execute_input":"2023-08-31T13:09:20.345651Z","iopub.status.idle":"2023-08-31T13:09:20.354408Z","shell.execute_reply.started":"2023-08-31T13:09:20.345609Z","shell.execute_reply":"2023-08-31T13:09:20.352981Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Split Funtion to Tokenize","metadata":{}},{"cell_type":"code","source":"text = \"God is Great! I won a lottery.\"\nprint(text.split())","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:24:47.411454Z","iopub.execute_input":"2023-08-31T13:24:47.411935Z","iopub.status.idle":"2023-08-31T13:24:47.418943Z","shell.execute_reply.started":"2023-08-31T13:24:47.411898Z","shell.execute_reply":"2023-08-31T13:24:47.417237Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"['God', 'is', 'Great!', 'I', 'won', 'a', 'lottery.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that some symblos are still there when we are converting sentences into words using split funtion, so better use word tokenize funtion or first preprocess the text and then Split","metadata":{}},{"cell_type":"code","source":"text = \"God is Great! I won a lottery.\"\ntext = re.sub('[^a-zA-Z]', ' ', text).lower()\nprint(text.split())","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:27:42.412030Z","iopub.execute_input":"2023-08-31T13:27:42.412487Z","iopub.status.idle":"2023-08-31T13:27:42.418586Z","shell.execute_reply.started":"2023-08-31T13:27:42.412452Z","shell.execute_reply":"2023-08-31T13:27:42.417237Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"['god', 'is', 'great', 'i', 'won', 'a', 'lottery']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"See Now there are no symbols \n### For Word tokenize\n- We can use preprocessing of text and then split.\n- Word tokenize(better to do Preprocessing before that)","metadata":{}},{"cell_type":"markdown","source":"## Sentence Tokenization\n- Split the sentences by ","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\ntext = \"God is Great! I won a lottery.\"\nprint(sent_tokenize(text))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:10:44.033578Z","iopub.execute_input":"2023-08-31T13:10:44.034159Z","iopub.status.idle":"2023-08-31T13:10:44.041483Z","shell.execute_reply.started":"2023-08-31T13:10:44.034117Z","shell.execute_reply":"2023-08-31T13:10:44.039945Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"['God is Great!', 'I won a lottery.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization using [Genism](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess)\n\nParameters\ndoc (str) – Input document.\n\ndeacc (bool, optional) – Remove accent marks from tokens using deaccent()?\n\nmin_len (int, optional) – Minimum length of token (inclusive). Shorter tokens are discarded.\n\nmax_len (int, optional) – Maximum length of token in result (inclusive). Longer tokens are discarded.","metadata":{}},{"cell_type":"code","source":"text = \"God is Great! I won a lottery.\"\nresult = gensim.utils.simple_preprocess(text, deacc=False, min_len=2, max_len=15)\nresult","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:36:40.177643Z","iopub.execute_input":"2023-08-31T13:36:40.178043Z","iopub.status.idle":"2023-08-31T13:36:40.187805Z","shell.execute_reply.started":"2023-08-31T13:36:40.178011Z","shell.execute_reply":"2023-08-31T13:36:40.186954Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['god', 'is', 'great', 'won', 'lottery']"},"metadata":{}}]}]}