{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",encoding='ISO-8859-1')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:49:21.569189Z","iopub.execute_input":"2023-09-05T06:49:21.570288Z","iopub.status.idle":"2023-09-05T06:49:21.607746Z","shell.execute_reply.started":"2023-09-05T06:49:21.570234Z","shell.execute_reply":"2023-09-05T06:49:21.606377Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        v1                                                 v2 Unnamed: 2  \\\n0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n1      ham                      Ok lar... Joking wif u oni...        NaN   \n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3      ham  U dun say so early hor... U c already then say...        NaN   \n4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n...    ...                                                ...        ...   \n5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n5571   ham                         Rofl. Its true to its name        NaN   \n\n     Unnamed: 3 Unnamed: 4  \n0           NaN        NaN  \n1           NaN        NaN  \n2           NaN        NaN  \n3           NaN        NaN  \n4           NaN        NaN  \n...         ...        ...  \n5567        NaN        NaN  \n5568        NaN        NaN  \n5569        NaN        NaN  \n5570        NaN        NaN  \n5571        NaN        NaN  \n\n[5572 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df=df[[\"v1\",\"v2\"]]\ndf.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],axis=1,inplace=True)\ndf.rename(columns={\"v1\":\"label\",\"v2\":\"message\"},inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:49:21.610257Z","iopub.execute_input":"2023-09-05T06:49:21.611214Z","iopub.status.idle":"2023-09-05T06:49:21.634463Z","shell.execute_reply.started":"2023-09-05T06:49:21.611169Z","shell.execute_reply":"2023-09-05T06:49:21.632609Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"     label                                            message\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n...    ...                                                ...\n5567  spam  This is the 2nd time we have tried 2 contact u...\n5568   ham              Will Ì_ b going to esplanade fr home?\n5569   ham  Pity, * was in mood for that. So...any other s...\n5570   ham  The guy did some bitching but I acted like i'd...\n5571   ham                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Lets see How data Looks Like ","metadata":{}},{"cell_type":"code","source":"for i in range(100,110):\n    print(f\"{df['message'][i]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:49:21.637760Z","iopub.execute_input":"2023-09-05T06:49:21.638862Z","iopub.status.idle":"2023-09-05T06:49:21.652283Z","shell.execute_reply.started":"2023-09-05T06:49:21.638785Z","shell.execute_reply":"2023-09-05T06:49:21.650812Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Okay name ur price as long as its legal! Wen can I pick them up? Y u ave x ams xx\n\nI'm still looking for a car to buy. And have not gone 4the driving test yet.\n\nAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n\nwow. You're right! I didn't mean to do that. I guess once i gave up on boston men and changed my search location to nyc, something changed. Cuz on my signin page it still says boston.\n\nUmma my life and vava umma love you lot dear\n\nThanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\n\nAight, I'll hit you up when I get some cash\n\nHow would my ip address test that considering my computer isn't a minecraft server\n\nI know! Grumpy old people. My mom was like you better not be lying. Then again I am always the one to play jokes...\n\nDont worry. I guess he's busy.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## We are getting text data , so we cant feed this data into final model for classification of is it a Spam mail or not.\n### We have to follow these steps for final model.\n- Perform  All kind of Text Preprocessing techniques to get Cleaned data, Because we cant provide the These data directly into Text2Vec Model, Which doesnt give proper Vector representation.\n- Combine this data along with \"label\" and divide them into train and testing data, Normal ration is 70:30, we can also include Validation data.\n- Then Build Different Classifiers and then Calculate accuracy,AUC Cuurve,ROC Curve,F1 Score,Precision,Recall and other Relevant Metrics.\n- Then Plot the Train and test data loss and Accuracy plot.\n- Compare with Different Models Accuracy and Loss,then Select the Best Model.\n- Hypertune the model using Grid seach,Random search.\n- Now Model is Ready.","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing\n-  Removing Punctuations.\n-  Removing Non-Alphabetic Characters(Removing Digits and other characters like /\\|\";.,)\n-  Removing Whitespaces.\n-  Lowercasing the text(the Text2vec Model treat it as differnt characters , if it's not converted).\n-  Tokenization(Word,Sentence)\n-  Stemming or Lemmatization\n-  Removing Stopwords.\n-  Joining all Words into Each sentence and Create Text_Corpus to Convert Text2Vec Using text2vec model.","metadata":{}},{"cell_type":"code","source":"## Import nltk library to do NLP tasks \n# (*) Will import all modules from that library\nimport nltk\n## For stopwords\nfrom nltk.corpus import *\n# For Stemming and Lemmatization Preprocessing Models\nfrom nltk.stem import *\n## For regex Modlue to find patterns and preprorcessing of text\nimport re\n## Install wordnet \n!pip install wordnet\n## Import wordnet for Lemmatization process to find meaningful word\nnltk.download('wordnet')\n## To unzip the corpora wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n!pip install gensim\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:49:21.653949Z","iopub.execute_input":"2023-09-05T06:49:21.654356Z","iopub.status.idle":"2023-09-05T06:50:03.567275Z","shell.execute_reply.started":"2023-09-05T06:49:21.654323Z","shell.execute_reply":"2023-09-05T06:50:03.565357Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Collecting wordnet\n  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting colorama==0.3.9 (from wordnet)\n  Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\nBuilding wheels for collected packages: wordnet\n  Building wheel for wordnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10520 sha256=7766c1d36160223fbbce47c9fc6cbdd1bbef0981b9672d8c5832cd70f76bb301\n  Stored in directory: /root/.cache/pip/wheels/c0/a1/e8/4649c8712033dcdbd1e64a0fc75216a5d1769665852c36b4f9\nSuccessfully built wordnet\nInstalling collected packages: colorama, wordnet\n  Attempting uninstall: colorama\n    Found existing installation: colorama 0.4.6\n    Uninstalling colorama-0.4.6:\n      Successfully uninstalled colorama-0.4.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbayesian-optimization 1.4.3 requires colorama>=0.4.6, but you have colorama 0.3.9 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed colorama-0.3.9 wordnet-0.0.1b2\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.1)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## For Text Preprocessing  with Stemming \n- Stemming Gives root word or each word , it may have meaning or not in the real world.\n- Removing Non-alphabetic Characters\n- lowercase all the Words\n- Split all the words in the senteces to individual words\n- Applyig Stemming for each word after checking if the word is present in the stpowords list or not\n- joining all the Words for Single Sentence\n- Storing each sentence in the list in each index position of list\n","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing of text with Lemmatization\n- Lemmatizationn gives the Root words of words which has meaning in the real world ie, valid words.\n- For this task only one thing has to change from the previous code , which is changing stemming into lemmatization\n\n - Changes \n    - change the stemmer of stemming into Lemmatizing stemmer which is WordNetLemmatizer(there are others , u can choose whichever giving good results)\n    - Changing the Stemming process to Lemmatization\n","metadata":{}},{"cell_type":"markdown","source":"# Funtion for Preprocessing text With Stemming  or Lemmatization  each word","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text, use_stemming=True, use_lemmatization=False):\n    corpus = []\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words(\"english\"))\n    \n    for i in range(len(text)):\n        # Remove non-alphanumeric characters and convert to lowercase\n        review = re.sub('[^a-zA-Z]', ' ', text[i]).lower()\n        \n        # Tokenize the text\n        words = nltk.word_tokenize(review)\n        # we can also use this words=review.split()\n        # we can use  gensim.utils.simple_preprocess() funtion\n        \n        # Apply stemming or lemmatization and remove stopwords\n        if use_stemming:\n            words = [stemmer.stem(word) for word in words if word not in stop_words]\n        elif use_lemmatization:\n            words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n        \n        # Join the processed words back into a sentence\n        review = \" \".join(words)\n        corpus.append(review)\n    \n    return corpus","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:03.569264Z","iopub.execute_input":"2023-09-05T06:50:03.569661Z","iopub.status.idle":"2023-09-05T06:50:03.587147Z","shell.execute_reply.started":"2023-09-05T06:50:03.569621Z","shell.execute_reply":"2023-09-05T06:50:03.585275Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## Call the funtion to get each corpus\nStemmed_corpus=preprocess_text(text=df[\"message\"], use_stemming=True, use_lemmatization=False)\nLemmatized_corpus=preprocess_text(text=df[\"message\"], use_stemming=False, use_lemmatization=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:03.588745Z","iopub.execute_input":"2023-09-05T06:50:03.589194Z","iopub.status.idle":"2023-09-05T06:50:12.016325Z","shell.execute_reply.started":"2023-09-05T06:50:03.589161Z","shell.execute_reply":"2023-09-05T06:50:12.014564Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"## To compare raw_corpus,Stemmed_corpus,Lemmatized corpus\nprint(f\"Raw Text  before  preprocessing of text and Lemmatization  is applied\\n{df['message'][10]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Stemming  is applied \\n {Stemmed_corpus[10]}\\n\\n\")\nprint(f\"Text after preprocessing of text and Lemmatization  is applied \\n {Lemmatized_corpus[10]}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.020472Z","iopub.execute_input":"2023-09-05T06:50:12.022409Z","iopub.status.idle":"2023-09-05T06:50:12.031481Z","shell.execute_reply.started":"2023-09-05T06:50:12.022339Z","shell.execute_reply":"2023-09-05T06:50:12.029916Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Raw Text  before  preprocessing of text and Lemmatization  is applied\nI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n\n\nText after preprocessing of text and Stemming  is applied \n gon na home soon want talk stuff anymor tonight k cri enough today\n\n\nText after preprocessing of text and Lemmatization  is applied \n gon na home soon want talk stuff anymore tonight k cried enough today\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Vectorization Techniques\n\n1. Bag of Words(Bow) or CountVectorizer                 - Skleran Library\n2. Term Frequency-Inverse Document Frequency (TF-IDF)   - Sklearn Library\n3. Word Embeddings                                      - Word2Vec, GloVe, FastText\n4. Word2vec                                             - Genism or Spacy\n     - A.CBOW \n     - B.Skipgram\n5. N-grams                                               - Nltk,\n5. BERT (Bidirectional Encoder Representations from Transformers) - Hugging face\n6. Feature extrators(Transformers)                       - Hugging face\n\nNote: We can also get word Embeddings by deep learning models by using Pytorch and Tensorflow Framework\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Text to Vectorization Techniques\n\n1. **[Bag of Words (BoW) or CountVectorizer](https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/)**\n   - Library: Scikit-learn (sklearn)\n   - Description: BoW represents text as a collection of words, ignoring their order and context. It creates a matrix where rows are documents, columns are unique words, and each cell represents the count of the word in the document.\n\n2. **[Term Frequency-Inverse Document Frequency (TF-IDF)](https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/)**\n   - Library: Scikit-learn (sklearn)\n   - Description: TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a corpus of documents. It's used to represent text as numerical values.\n\n3. **Word Embeddings**\n   - Description: Word embeddings are dense vector representations of words that capture semantic meaning. Several pre-trained models are available:\n     - **[Word2Vec](https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/)**: Trained using techniques like CBOW and Skipgram.\n       - Libraries: [Gensim](https://radimrehurek.com/gensim/models/word2vec.html) or [Spacy](https://www.kaggle.com/code/farsanas/spacy-word2vec)\n     - **GloVe**: Global Vectors for Word Representation\n     - **FastText**: Embeds subword information along with words.\n\n4. **[Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)**\n   - Libraries: Gensim or Spacy\n   - Description: Word2Vec is a specific word embedding model that creates vector representations of words based on their co-occurrence patterns. Variants include Continuous Bag of Words (CBOW) and Skipgram models.\n\n5. **[N-grams](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/)**\n   - Description: N-grams are contiguous sequences of N items (usually words) in a text. Libraries like NLTK can be used to extract and work with N-grams.\n\n6. **[BERT (Bidirectional Encoder Representations from Transformers)](https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f)**\n   - Library: Hugging Face Transformers\n   - Description: BERT is a powerful transformer-based model that captures contextual information from text. It is often used for various NLP tasks, including text classification, named entity recognition, and more.\n\n7. **[Feature Extractors (Transformers)](https://huggingface.co/tasks/feature-extraction)**\n   - Library: Hugging Face Transformers\n   - Description: Transformers like BERT, GPT-2, and others can be fine-tuned for specific NLP tasks or used to extract features from text.\n\n#### Note :Additionally, word embeddings can also be obtained using deep learning models built with frameworks like PyTorch and TensorFlow.\n\nThese techniques and libraries are essential tools for various NLP tasks, from text classification and sentiment analysis to machine translation and language generation. The choice of technique often depends on the specific task and the characteristics of the data being processed.\n","metadata":{}},{"cell_type":"markdown","source":"## Note: In this Notebook i have only implemented Bag of words model and TF-IDF Vectorizer to convert into Text into Vector , in the Next Notebooks , i am  gonna implement all of above methods","metadata":{}},{"cell_type":"markdown","source":"### To Get N-grams using Nltk","metadata":{}},{"cell_type":"code","source":"from nltk import ngrams\n\nsentence = 'this is a foo bar sentences and I want to ngramize it'\n\n## Specify the n , Here we are getting Tri-gram \nn = 3\nsixgrams = ngrams(sentence.split(), 3)\n\nfor grams in sixgrams:\n  print(grams)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now the Text data is Preprocessed and we need to convert this data into Vectors , then  only we can Give this data into final model for classification.\n\n### Sklearn  submodule  [sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) gathers utilities to build feature vectors from text documents. \n- Which has 4 models\n    - [CountVectorizer() or Bag of Words (BoW) Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) - Convert a collection of text documents to a matrix of token counts.\n\n    - [HashingVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) - Convert a collection of text documents to a matrix of token occurrences.\n\n    - [TfidfTransformer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) - Transform a count matrix to a normalized tf or tf-idf representation.\n\n    - [TfidfVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) - Convert a collection of raw documents to a matrix of TF-IDF features.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import *\n##Define the vectorizer\nvectorizer = CountVectorizer()\n## we can tune the various hyperparameters in the above funtion , lets continue with default parameters\n##fit the text to the model\nX = vectorizer.fit_transform(Stemmed_corpus)\n##get the feature names\nvectorizer.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.033319Z","iopub.execute_input":"2023-09-05T06:50:12.033801Z","iopub.status.idle":"2023-09-05T06:50:12.178253Z","shell.execute_reply.started":"2023-09-05T06:50:12.033747Z","shell.execute_reply":"2023-09-05T06:50:12.177094Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array(['aa', 'aah', 'aaniy', ..., 'zouk', 'zs', 'zyada'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"print(X.toarray())","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.180320Z","iopub.execute_input":"2023-09-05T06:50:12.181269Z","iopub.status.idle":"2023-09-05T06:50:12.315035Z","shell.execute_reply.started":"2023-09-05T06:50:12.181227Z","shell.execute_reply":"2023-09-05T06:50:12.313483Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"## This is the shape of the feature vectors\nprint(X.shape)\n## first value in above shape is the len of corpus we have provided(Rows)\nprint(len(Stemmed_corpus))\n## Second value in the shape is the unique feature names we have extracted from the text\nprint(len(vectorizer.get_feature_names_out()))","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.316805Z","iopub.execute_input":"2023-09-05T06:50:12.317256Z","iopub.status.idle":"2023-09-05T06:50:12.332682Z","shell.execute_reply.started":"2023-09-05T06:50:12.317217Z","shell.execute_reply":"2023-09-05T06:50:12.331101Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(5572, 6217)\n5572\n6217\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Also we can set the Max features we are getting from the text in the parameter Ex:max_features=500 and also if we want differnt ngram in the model , that also we can select","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(max_features=5000,ngram_range=(2,2))\n## we can tune the various hyperparameters in the above funtion , lets continue with default parameters\n##fit the text to the model\nX = vectorizer.fit_transform(Stemmed_corpus)\n##get the feature names\nvectorizer.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.334610Z","iopub.execute_input":"2023-09-05T06:50:12.335414Z","iopub.status.idle":"2023-09-05T06:50:12.638150Z","shell.execute_reply.started":"2023-09-05T06:50:12.335354Z","shell.execute_reply":"2023-09-05T06:50:12.636519Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array(['aathi dear', 'aathi love', 'abl get', ..., 'yup ok', 'yup thk',\n       'zed profit'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Lets Create a funtion to get feature vectors from the text  for different preprocessed corpus","metadata":{}},{"cell_type":"code","source":"def Text_to_vectors(vectorizer=None, text=None, use_stemming=True, use_lemmatization=False, max_features=None, ngram_range=(1, 1)):\n    if vectorizer is None:\n        # Default to CountVectorizer(BOW Model) if no vectorizer is provided\n        vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n    elif isinstance(vectorizer, str):\n        # Allow passing the name of the vectorizer as a string\n        if vectorizer.lower() == \"countvectorizer\":\n            vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n        elif vectorizer.lower() == \"tfidfvectorizer\":\n            vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)            \n        # Add more options for custom vectorizers here\n\n    # Preprocess the text data\n    corpus = preprocess_text(text, use_stemming=use_stemming, use_lemmatization=use_lemmatization)\n\n    # Fit the corpus to the vectorizer and convert to array\n    X = vectorizer.fit_transform(corpus).toarray()\n    feature_names = vectorizer.get_feature_names_out()\n    return X, feature_names, corpus","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.640144Z","iopub.execute_input":"2023-09-05T06:50:12.640708Z","iopub.status.idle":"2023-09-05T06:50:12.655204Z","shell.execute_reply.started":"2023-09-05T06:50:12.640668Z","shell.execute_reply":"2023-09-05T06:50:12.653140Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Example usage with a custom vectorizer (e.g., TfidfVectorizer)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create a custom vectorizer (TfidfVectorizer in this case)\ncustom_vectorizer = TfidfVectorizer(max_features=6000, ngram_range=(2, 2))\n\n# Call the Text_to_vectors function with no vectrorizer ie Count vectorizer\nX, feature_names, corpus = Text_to_vectors(vectorizer=None, text=df[\"message\"], use_stemming=False, max_features=5000, ngram_range=(2, 2))\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:12.658966Z","iopub.execute_input":"2023-09-05T06:50:12.659983Z","iopub.status.idle":"2023-09-05T06:50:14.513465Z","shell.execute_reply.started":"2023-09-05T06:50:12.659910Z","shell.execute_reply":"2023-09-05T06:50:14.511549Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(5572, 5000)"},"metadata":{}}]},{"cell_type":"code","source":"# Call the Text_to_vectors function with the custom vectorizer\nX, feature_names, corpus = Text_to_vectors(vectorizer=custom_vectorizer, text=df[\"message\"], use_stemming=False, max_features=5000, ngram_range=(2, 2))\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:14.515617Z","iopub.execute_input":"2023-09-05T06:50:14.516177Z","iopub.status.idle":"2023-09-05T06:50:16.408305Z","shell.execute_reply.started":"2023-09-05T06:50:14.516133Z","shell.execute_reply":"2023-09-05T06:50:16.407041Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(5572, 6000)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Convert Target Column into Numerical  Representation by pandas dummies(One-Hot Encodig) method","metadata":{}},{"cell_type":"code","source":"## We are converting target column from text format to numerical representation using pd_get dummies method\ntarget=pd.get_dummies(data=df[\"label\"],drop_first=True)\n# Target Column\nprint(target,\"\\n\\n\",target.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:16.409790Z","iopub.execute_input":"2023-09-05T06:50:16.410181Z","iopub.status.idle":"2023-09-05T06:50:16.432727Z","shell.execute_reply.started":"2023-09-05T06:50:16.410148Z","shell.execute_reply":"2023-09-05T06:50:16.431289Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"      spam\n0        0\n1        0\n2        1\n3        0\n4        0\n...    ...\n5567     1\n5568     0\n5569     0\n5570     0\n5571     0\n\n[5572 rows x 1 columns] \n\n spam\n0       4825\n1        747\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here spam is Converted as 1 and ham is Converted as 0 , we can check for both spam and ham target column by deleting other column,Based on the accuracy of final model.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# import Necessary Libraries to build model and to check performance of Model\nfrom sklearn.pipeline import Pipeline\n# To import Logistic regression Models\nfrom sklearn.linear_model import *\n#for Metrics\nfrom sklearn.metrics import *\n## To train_test_split method\nfrom sklearn.model_selection import *\n# For Naive Bayes method\nfrom sklearn.naive_bayes import *\n# For Support vector Machine\nfrom sklearn.svm import SVC\n## For logistic Regression\nfrom sklearn.linear_model import *\n## For Ensemble models\nfrom sklearn.ensemble import *\n## For Text to Vectorizer models\nfrom sklearn.feature_extraction.text import *\n## To Split the data as train and test\nfrom sklearn.model_selection import *","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:16.434721Z","iopub.execute_input":"2023-09-05T06:50:16.435220Z","iopub.status.idle":"2023-09-05T06:50:16.803295Z","shell.execute_reply.started":"2023-09-05T06:50:16.435176Z","shell.execute_reply":"2023-09-05T06:50:16.801569Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Create a Dataframe to store all the results and also Create [Sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n\nThe purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n\n#### [What is Pipleline and why do we need it ](https://stackoverflow.com/questions/33091376/what-is-exactly-sklearn-pipeline-pipeline)","metadata":{}},{"cell_type":"code","source":"\n\n# Initialize an empty DataFrame to store the results\nresults_df = pd.DataFrame(columns=[\"Experiment\", \"Vectorizer\", \"Model\", \"Preprocessing\", \"Parameters\",\n                                    \"Training Accuracy\", \"Testing Accuracy\",\n                                    \"F1 Micro\", \"F1 Macro\", \"F1 Weighted\", \"Recall Micro\", \"Recall Macro\",\n                                    \"Recall Weighted\", \"Precision Micro\", \"Precision Macro\", \"Precision Weighted\",\n                                    \"Confusion Matrix\"])\n\n# Define different vectorizers\nvectorizers = [\n    (\"CountVectorizer\", CountVectorizer()),\n    (\"TfidfVectorizer\", TfidfVectorizer()),\n    # Add more vectorizers here\n]\n\n# Define different models\nmodels = [\n    (\"MultinomialNB\", MultinomialNB()),\n    (\"LogisticRegression\", LogisticRegression()),\n    (\"SVM\", SVC()),\n    (\"RandomForest\", RandomForestClassifier()),\n    # Add more models here\n]\n\n# Define different preprocessing options\npreprocessing_options = [\n    {\"use_stemming\": True, \"use_lemmatization\": False},\n    {\"use_stemming\": False, \"use_lemmatization\": True},\n    # Add more preprocessing options here\n]\n\n# Define different parameter configurations\nparameter_configs = [\n    {\"model_name\": \"MultinomialNB\", \"vectorizer_name\": \"CountVectorizer\", \"max_features\": 5000, \"ngram_range\": (1, 1)},\n    {\"model_name\": \"LogisticRegression\", \"vectorizer_name\": \"TfidfVectorizer\", \"max_features\": 5000, \"ngram_range\": (2, 2)},\n    # Add more parameter configurations here\n]\n\n\n## Assign dependent and independent data to these variables\ntext = df[\"message\"]\ntarget = target \n\n# Iterate over different combinations of vectorizers, models, preprocessing options, and parameters\nfor vectorizer_name, vectorizer in vectorizers:\n    for model_name, model in models:\n        for preprocessing_option in preprocessing_options:\n            for parameter_config in parameter_configs:\n                # Retrieve parameters\n                use_stemming = preprocessing_option[\"use_stemming\"]\n                use_lemmatization = preprocessing_option[\"use_lemmatization\"]\n                max_features = parameter_config[\"max_features\"]\n                ngram_range = parameter_config[\"ngram_range\"]\n\n                # Create a combined name for this experiment\n                experiment_name = f\"{vectorizer_name}_{model_name}_stemming{use_stemming}_lemmatization{use_stemming}_maxfeatures{max_features}_ngram{ngram_range}\"\n\n                # Preprocess the text data\n                corpus = preprocess_text(text, use_stemming=use_stemming, use_lemmatization=use_lemmatization)\n\n                # Split the data into train and test sets (you can adjust this as needed)\n                X_train, X_test, y_train, y_test = train_test_split(corpus, target, test_size=0.2, random_state=42)\n\n                # Create and configure the vectorizer\n                vectorizer.set_params(max_features=max_features, ngram_range=ngram_range)\n                X_train_vectorized = vectorizer.fit_transform(X_train)\n                X_test_vectorized = vectorizer.transform(X_test)\n\n                # Create and configure the model\n                model.set_params()\n\n                # Build a pipeline with vectorizer and model\n                pipeline = Pipeline([\n                    (\"vectorizer\", vectorizer),\n                    (\"model\", model)\n                ])\n\n                # Train the model\n                pipeline.fit(X_train, y_train)\n\n                # Calculate training accuracy\n                y_train_pred = pipeline.predict(X_train)\n                training_accuracy = accuracy_score(y_train, y_train_pred)\n\n                # Make predictions and evaluate the model on the test data\n                y_pred = pipeline.predict(X_test)\n                testing_accuracy = accuracy_score(y_test, y_pred)\n\n                # Calculate evaluation metrics manually\n                f1_micro = f1_score(y_test, y_pred, average='micro')\n                f1_macro = f1_score(y_test, y_pred, average='macro')\n                f1_weighted = f1_score(y_test, y_pred, average='weighted')\n                recall_micro = recall_score(y_test, y_pred, average='micro')\n                recall_macro = recall_score(y_test, y_pred, average='macro')\n                recall_weighted = recall_score(y_test, y_pred, average='weighted')\n                precision_micro = precision_score(y_test, y_pred, average='micro')\n                precision_macro = precision_score(y_test, y_pred, average='macro')\n                precision_weighted = precision_score(y_test, y_pred, average='weighted')\n                cm = confusion_matrix(y_test, y_pred)\n\n                # Append the metrics to the results DataFrame\n                new_row = {\n                    \"Experiment\": experiment_name,\n                    \"Vectorizer\": vectorizer_name,\n                    \"Model\": model_name,\n                    \"Preprocessing\": f\"Stemming={use_stemming}, Lemmatization={use_lemmatization}\",\n                    \"Parameters\": f\"Max Features={max_features}, Ngram Range={ngram_range}\",\n                    \"Training Accuracy\": training_accuracy,\n                    \"Testing Accuracy\": testing_accuracy,\n                    \"F1 Micro\": f1_micro,\n                    \"F1 Macro\": f1_macro,\n                    \"F1 Weighted\": f1_weighted,\n                    \"Recall Micro\": recall_micro,\n                    \"Recall Macro\": recall_macro,\n                    \"Recall Weighted\": recall_weighted,\n                    \"Precision Micro\": precision_micro,\n                    \"Precision Macro\": precision_macro,\n                    \"Precision Weighted\": precision_weighted,\n                    \"Confusion Matrix\": cm\n                }\n\n                results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n\n# Display the results DataFrame\nprint(results_df)\n\n# Optionally, save the DataFrame to a CSV file\n#results_df.to_csv(\"experiment_results.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:50:16.808953Z","iopub.execute_input":"2023-09-05T06:50:16.809418Z","iopub.status.idle":"2023-09-05T06:52:43.823795Z","shell.execute_reply.started":"2023-09-05T06:50:16.809385Z","shell.execute_reply":"2023-09-05T06:52:43.821909Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","output_type":"stream"},{"name":"stdout","text":"                                           Experiment       Vectorizer  \\\n0   CountVectorizer_MultinomialNB_stemmingTrue_lem...  CountVectorizer   \n1   CountVectorizer_MultinomialNB_stemmingTrue_lem...  CountVectorizer   \n2   CountVectorizer_MultinomialNB_stemmingFalse_le...  CountVectorizer   \n3   CountVectorizer_MultinomialNB_stemmingFalse_le...  CountVectorizer   \n4   CountVectorizer_LogisticRegression_stemmingTru...  CountVectorizer   \n5   CountVectorizer_LogisticRegression_stemmingTru...  CountVectorizer   \n6   CountVectorizer_LogisticRegression_stemmingFal...  CountVectorizer   \n7   CountVectorizer_LogisticRegression_stemmingFal...  CountVectorizer   \n8   CountVectorizer_SVM_stemmingTrue_lemmatization...  CountVectorizer   \n9   CountVectorizer_SVM_stemmingTrue_lemmatization...  CountVectorizer   \n10  CountVectorizer_SVM_stemmingFalse_lemmatizatio...  CountVectorizer   \n11  CountVectorizer_SVM_stemmingFalse_lemmatizatio...  CountVectorizer   \n12  CountVectorizer_RandomForest_stemmingTrue_lemm...  CountVectorizer   \n13  CountVectorizer_RandomForest_stemmingTrue_lemm...  CountVectorizer   \n14  CountVectorizer_RandomForest_stemmingFalse_lem...  CountVectorizer   \n15  CountVectorizer_RandomForest_stemmingFalse_lem...  CountVectorizer   \n16  TfidfVectorizer_MultinomialNB_stemmingTrue_lem...  TfidfVectorizer   \n17  TfidfVectorizer_MultinomialNB_stemmingTrue_lem...  TfidfVectorizer   \n18  TfidfVectorizer_MultinomialNB_stemmingFalse_le...  TfidfVectorizer   \n19  TfidfVectorizer_MultinomialNB_stemmingFalse_le...  TfidfVectorizer   \n20  TfidfVectorizer_LogisticRegression_stemmingTru...  TfidfVectorizer   \n21  TfidfVectorizer_LogisticRegression_stemmingTru...  TfidfVectorizer   \n22  TfidfVectorizer_LogisticRegression_stemmingFal...  TfidfVectorizer   \n23  TfidfVectorizer_LogisticRegression_stemmingFal...  TfidfVectorizer   \n24  TfidfVectorizer_SVM_stemmingTrue_lemmatization...  TfidfVectorizer   \n25  TfidfVectorizer_SVM_stemmingTrue_lemmatization...  TfidfVectorizer   \n26  TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...  TfidfVectorizer   \n27  TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...  TfidfVectorizer   \n28  TfidfVectorizer_RandomForest_stemmingTrue_lemm...  TfidfVectorizer   \n29  TfidfVectorizer_RandomForest_stemmingTrue_lemm...  TfidfVectorizer   \n30  TfidfVectorizer_RandomForest_stemmingFalse_lem...  TfidfVectorizer   \n31  TfidfVectorizer_RandomForest_stemmingFalse_lem...  TfidfVectorizer   \n\n                 Model                       Preprocessing  \\\n0        MultinomialNB  Stemming=True, Lemmatization=False   \n1        MultinomialNB  Stemming=True, Lemmatization=False   \n2        MultinomialNB  Stemming=False, Lemmatization=True   \n3        MultinomialNB  Stemming=False, Lemmatization=True   \n4   LogisticRegression  Stemming=True, Lemmatization=False   \n5   LogisticRegression  Stemming=True, Lemmatization=False   \n6   LogisticRegression  Stemming=False, Lemmatization=True   \n7   LogisticRegression  Stemming=False, Lemmatization=True   \n8                  SVM  Stemming=True, Lemmatization=False   \n9                  SVM  Stemming=True, Lemmatization=False   \n10                 SVM  Stemming=False, Lemmatization=True   \n11                 SVM  Stemming=False, Lemmatization=True   \n12        RandomForest  Stemming=True, Lemmatization=False   \n13        RandomForest  Stemming=True, Lemmatization=False   \n14        RandomForest  Stemming=False, Lemmatization=True   \n15        RandomForest  Stemming=False, Lemmatization=True   \n16       MultinomialNB  Stemming=True, Lemmatization=False   \n17       MultinomialNB  Stemming=True, Lemmatization=False   \n18       MultinomialNB  Stemming=False, Lemmatization=True   \n19       MultinomialNB  Stemming=False, Lemmatization=True   \n20  LogisticRegression  Stemming=True, Lemmatization=False   \n21  LogisticRegression  Stemming=True, Lemmatization=False   \n22  LogisticRegression  Stemming=False, Lemmatization=True   \n23  LogisticRegression  Stemming=False, Lemmatization=True   \n24                 SVM  Stemming=True, Lemmatization=False   \n25                 SVM  Stemming=True, Lemmatization=False   \n26                 SVM  Stemming=False, Lemmatization=True   \n27                 SVM  Stemming=False, Lemmatization=True   \n28        RandomForest  Stemming=True, Lemmatization=False   \n29        RandomForest  Stemming=True, Lemmatization=False   \n30        RandomForest  Stemming=False, Lemmatization=True   \n31        RandomForest  Stemming=False, Lemmatization=True   \n\n                               Parameters  Training Accuracy  \\\n0   Max Features=5000, Ngram Range=(1, 1)           0.992147   \n1   Max Features=5000, Ngram Range=(2, 2)           0.987660   \n2   Max Features=5000, Ngram Range=(1, 1)           0.990577   \n3   Max Features=5000, Ngram Range=(2, 2)           0.986089   \n4   Max Features=5000, Ngram Range=(1, 1)           0.996186   \n5   Max Features=5000, Ngram Range=(2, 2)           0.973525   \n6   Max Features=5000, Ngram Range=(1, 1)           0.995737   \n7   Max Features=5000, Ngram Range=(2, 2)           0.973076   \n8   Max Features=5000, Ngram Range=(1, 1)           0.996410   \n9   Max Features=5000, Ngram Range=(2, 2)           0.985192   \n10  Max Features=5000, Ngram Range=(1, 1)           0.995513   \n11  Max Features=5000, Ngram Range=(2, 2)           0.984519   \n12  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n13  Max Features=5000, Ngram Range=(2, 2)           0.995961   \n14  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n15  Max Features=5000, Ngram Range=(2, 2)           0.995064   \n16  Max Features=5000, Ngram Range=(1, 1)           0.981153   \n17  Max Features=5000, Ngram Range=(2, 2)           0.974647   \n18  Max Features=5000, Ngram Range=(1, 1)           0.982499   \n19  Max Features=5000, Ngram Range=(2, 2)           0.973749   \n20  Max Features=5000, Ngram Range=(1, 1)           0.973749   \n21  Max Features=5000, Ngram Range=(2, 2)           0.924389   \n22  Max Features=5000, Ngram Range=(1, 1)           0.973300   \n23  Max Features=5000, Ngram Range=(2, 2)           0.923491   \n24  Max Features=5000, Ngram Range=(1, 1)           0.997756   \n25  Max Features=5000, Ngram Range=(2, 2)           0.995288   \n26  Max Features=5000, Ngram Range=(1, 1)           0.997532   \n27  Max Features=5000, Ngram Range=(2, 2)           0.993942   \n28  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n29  Max Features=5000, Ngram Range=(2, 2)           0.996186   \n30  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n31  Max Features=5000, Ngram Range=(2, 2)           0.995064   \n\n    Testing Accuracy  F1 Micro  F1 Macro  F1 Weighted  Recall Micro  \\\n0           0.983857  0.983857  0.964941     0.983764      0.983857   \n1           0.966816  0.966816  0.921793     0.965166      0.966816   \n2           0.982063  0.982063  0.961046     0.981960      0.982063   \n3           0.967713  0.967713  0.924150     0.966166      0.967713   \n4           0.977578  0.977578  0.948152     0.976703      0.977578   \n5           0.943498  0.943498  0.851283     0.936881      0.943498   \n6           0.977578  0.977578  0.948152     0.976703      0.977578   \n7           0.944395  0.944395  0.854186     0.938017      0.944395   \n8           0.980269  0.980269  0.954796     0.979600      0.980269   \n9           0.949776  0.949776  0.871147     0.944720      0.949776   \n10          0.978475  0.978475  0.950687     0.977745      0.978475   \n11          0.950673  0.950673  0.873901     0.945819      0.950673   \n12          0.980269  0.980269  0.954796     0.979600      0.980269   \n13          0.965919  0.965919  0.918897     0.964036      0.965919   \n14          0.975785  0.975785  0.943651     0.974754      0.975785   \n15          0.966816  0.966816  0.921793     0.965166      0.966816   \n16          0.965919  0.965919  0.917826     0.963777      0.965919   \n17          0.954260  0.954260  0.885521     0.950362      0.954260   \n18          0.965919  0.965919  0.917826     0.963777      0.965919   \n19          0.955157  0.955157  0.888155     0.951430      0.955157   \n20          0.957848  0.957848  0.898027     0.955116      0.957848   \n21          0.910314  0.910314  0.727820     0.890725      0.910314   \n22          0.956951  0.956951  0.895510     0.954076      0.956951   \n23          0.913901  0.913901  0.743275     0.896257      0.913901   \n24          0.981166  0.981166  0.957246     0.980621      0.981166   \n25          0.960538  0.960538  0.903572     0.957746      0.960538   \n26          0.978475  0.978475  0.950989     0.977817      0.978475   \n27          0.959641  0.959641  0.901047     0.956705      0.959641   \n28          0.977578  0.977578  0.948152     0.976703      0.977578   \n29          0.967713  0.967713  0.923662     0.966049      0.967713   \n30          0.976682  0.976682  0.945909     0.975730      0.976682   \n31          0.965022  0.965022  0.917033     0.963155      0.965022   \n\n    Recall Macro  Recall Weighted  Precision Micro  Precision Macro  \\\n0       0.959706         0.983857         0.983857         0.970351   \n1       0.885112         0.966816         0.966816         0.970327   \n2       0.955855         0.982063         0.982063         0.966411   \n3       0.888446         0.967713         0.967713         0.970917   \n4       0.916667         0.977578         0.977578         0.987374   \n5       0.790000         0.943498         0.943498         0.969358   \n6       0.916667         0.977578         0.977578         0.987374   \n7       0.793333         0.944395         0.944395         0.969815   \n8       0.926667         0.980269         0.980269         0.988855   \n9       0.813333         0.949776         0.949776         0.972576   \n10      0.922815         0.978475         0.978475         0.984442   \n11      0.816667         0.950673         0.950673         0.973039   \n12      0.926667         0.980269         0.980269         0.988855   \n13      0.878964         0.965919         0.965919         0.973361   \n14      0.910000         0.975785         0.975785         0.986391   \n15      0.885112         0.966816         0.966816         0.970327   \n16      0.873333         0.965919         0.965919         0.981057   \n17      0.832815         0.954260         0.954260         0.970395   \n18      0.873333         0.965919         0.965919         0.981057   \n19      0.836149         0.955157         0.955157         0.970912   \n20      0.854594         0.957848         0.957848         0.960568   \n21      0.669482         0.910314         0.910314         0.943818   \n22      0.851261         0.956951         0.956951         0.959928   \n23      0.682815         0.913901         0.913901         0.946218   \n24      0.932815         0.981166         0.981166         0.986021   \n25      0.856149         0.960538         0.960538         0.974020   \n26      0.925630         0.978475         0.978475         0.981140   \n27      0.852815         0.959641         0.959641         0.973502   \n28      0.916667         0.977578         0.977578         0.987374   \n29      0.885630         0.967713         0.967713         0.974474   \n30      0.913333         0.976682         0.976682         0.986882   \n31      0.878446         0.965022         0.965022         0.969143   \n\n    Precision Weighted       Confusion Matrix  \n0             0.983725  [[958, 7], [11, 139]]  \n1             0.967064  [[962, 3], [34, 116]]  \n2             0.981911  [[957, 8], [12, 138]]  \n3             0.967933  [[962, 3], [33, 117]]  \n4             0.978145  [[965, 0], [25, 125]]  \n5             0.946960   [[965, 0], [63, 87]]  \n6             0.978145  [[965, 0], [25, 125]]  \n7             0.947752   [[965, 0], [62, 88]]  \n8             0.980709  [[965, 0], [22, 128]]  \n9             0.952530   [[965, 0], [56, 94]]  \n10            0.978781  [[964, 1], [23, 127]]  \n11            0.953332   [[965, 0], [55, 95]]  \n12            0.980709  [[965, 0], [22, 128]]  \n13            0.966492  [[963, 2], [36, 114]]  \n14            0.976444  [[965, 0], [27, 123]]  \n15            0.967064  [[962, 3], [34, 116]]  \n16            0.967210  [[965, 0], [38, 112]]  \n17            0.955992  [[964, 1], [50, 100]]  \n18            0.967210  [[965, 0], [38, 112]]  \n19            0.956817  [[964, 1], [49, 101]]  \n20            0.958085  [[961, 4], [43, 107]]  \n21            0.916809   [[964, 1], [99, 51]]  \n22            0.957217  [[961, 4], [44, 106]]  \n23            0.919959   [[964, 1], [95, 55]]  \n24            0.981382  [[964, 1], [20, 130]]  \n25            0.961798  [[964, 1], [43, 107]]  \n26            0.978600  [[963, 2], [22, 128]]  \n27            0.960964  [[964, 1], [44, 106]]  \n28            0.978145  [[965, 0], [25, 125]]  \n29            0.968205  [[963, 2], [34, 116]]  \n30            0.977293  [[965, 0], [26, 124]]  \n31            0.965331  [[962, 3], [36, 114]]  \n","output_type":"stream"}]},{"cell_type":"code","source":"results_df","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:52:43.826236Z","iopub.execute_input":"2023-09-05T06:52:43.826815Z","iopub.status.idle":"2023-09-05T06:52:43.916905Z","shell.execute_reply.started":"2023-09-05T06:52:43.826761Z","shell.execute_reply":"2023-09-05T06:52:43.915149Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                           Experiment       Vectorizer  \\\n0   CountVectorizer_MultinomialNB_stemmingTrue_lem...  CountVectorizer   \n1   CountVectorizer_MultinomialNB_stemmingTrue_lem...  CountVectorizer   \n2   CountVectorizer_MultinomialNB_stemmingFalse_le...  CountVectorizer   \n3   CountVectorizer_MultinomialNB_stemmingFalse_le...  CountVectorizer   \n4   CountVectorizer_LogisticRegression_stemmingTru...  CountVectorizer   \n5   CountVectorizer_LogisticRegression_stemmingTru...  CountVectorizer   \n6   CountVectorizer_LogisticRegression_stemmingFal...  CountVectorizer   \n7   CountVectorizer_LogisticRegression_stemmingFal...  CountVectorizer   \n8   CountVectorizer_SVM_stemmingTrue_lemmatization...  CountVectorizer   \n9   CountVectorizer_SVM_stemmingTrue_lemmatization...  CountVectorizer   \n10  CountVectorizer_SVM_stemmingFalse_lemmatizatio...  CountVectorizer   \n11  CountVectorizer_SVM_stemmingFalse_lemmatizatio...  CountVectorizer   \n12  CountVectorizer_RandomForest_stemmingTrue_lemm...  CountVectorizer   \n13  CountVectorizer_RandomForest_stemmingTrue_lemm...  CountVectorizer   \n14  CountVectorizer_RandomForest_stemmingFalse_lem...  CountVectorizer   \n15  CountVectorizer_RandomForest_stemmingFalse_lem...  CountVectorizer   \n16  TfidfVectorizer_MultinomialNB_stemmingTrue_lem...  TfidfVectorizer   \n17  TfidfVectorizer_MultinomialNB_stemmingTrue_lem...  TfidfVectorizer   \n18  TfidfVectorizer_MultinomialNB_stemmingFalse_le...  TfidfVectorizer   \n19  TfidfVectorizer_MultinomialNB_stemmingFalse_le...  TfidfVectorizer   \n20  TfidfVectorizer_LogisticRegression_stemmingTru...  TfidfVectorizer   \n21  TfidfVectorizer_LogisticRegression_stemmingTru...  TfidfVectorizer   \n22  TfidfVectorizer_LogisticRegression_stemmingFal...  TfidfVectorizer   \n23  TfidfVectorizer_LogisticRegression_stemmingFal...  TfidfVectorizer   \n24  TfidfVectorizer_SVM_stemmingTrue_lemmatization...  TfidfVectorizer   \n25  TfidfVectorizer_SVM_stemmingTrue_lemmatization...  TfidfVectorizer   \n26  TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...  TfidfVectorizer   \n27  TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...  TfidfVectorizer   \n28  TfidfVectorizer_RandomForest_stemmingTrue_lemm...  TfidfVectorizer   \n29  TfidfVectorizer_RandomForest_stemmingTrue_lemm...  TfidfVectorizer   \n30  TfidfVectorizer_RandomForest_stemmingFalse_lem...  TfidfVectorizer   \n31  TfidfVectorizer_RandomForest_stemmingFalse_lem...  TfidfVectorizer   \n\n                 Model                       Preprocessing  \\\n0        MultinomialNB  Stemming=True, Lemmatization=False   \n1        MultinomialNB  Stemming=True, Lemmatization=False   \n2        MultinomialNB  Stemming=False, Lemmatization=True   \n3        MultinomialNB  Stemming=False, Lemmatization=True   \n4   LogisticRegression  Stemming=True, Lemmatization=False   \n5   LogisticRegression  Stemming=True, Lemmatization=False   \n6   LogisticRegression  Stemming=False, Lemmatization=True   \n7   LogisticRegression  Stemming=False, Lemmatization=True   \n8                  SVM  Stemming=True, Lemmatization=False   \n9                  SVM  Stemming=True, Lemmatization=False   \n10                 SVM  Stemming=False, Lemmatization=True   \n11                 SVM  Stemming=False, Lemmatization=True   \n12        RandomForest  Stemming=True, Lemmatization=False   \n13        RandomForest  Stemming=True, Lemmatization=False   \n14        RandomForest  Stemming=False, Lemmatization=True   \n15        RandomForest  Stemming=False, Lemmatization=True   \n16       MultinomialNB  Stemming=True, Lemmatization=False   \n17       MultinomialNB  Stemming=True, Lemmatization=False   \n18       MultinomialNB  Stemming=False, Lemmatization=True   \n19       MultinomialNB  Stemming=False, Lemmatization=True   \n20  LogisticRegression  Stemming=True, Lemmatization=False   \n21  LogisticRegression  Stemming=True, Lemmatization=False   \n22  LogisticRegression  Stemming=False, Lemmatization=True   \n23  LogisticRegression  Stemming=False, Lemmatization=True   \n24                 SVM  Stemming=True, Lemmatization=False   \n25                 SVM  Stemming=True, Lemmatization=False   \n26                 SVM  Stemming=False, Lemmatization=True   \n27                 SVM  Stemming=False, Lemmatization=True   \n28        RandomForest  Stemming=True, Lemmatization=False   \n29        RandomForest  Stemming=True, Lemmatization=False   \n30        RandomForest  Stemming=False, Lemmatization=True   \n31        RandomForest  Stemming=False, Lemmatization=True   \n\n                               Parameters  Training Accuracy  \\\n0   Max Features=5000, Ngram Range=(1, 1)           0.992147   \n1   Max Features=5000, Ngram Range=(2, 2)           0.987660   \n2   Max Features=5000, Ngram Range=(1, 1)           0.990577   \n3   Max Features=5000, Ngram Range=(2, 2)           0.986089   \n4   Max Features=5000, Ngram Range=(1, 1)           0.996186   \n5   Max Features=5000, Ngram Range=(2, 2)           0.973525   \n6   Max Features=5000, Ngram Range=(1, 1)           0.995737   \n7   Max Features=5000, Ngram Range=(2, 2)           0.973076   \n8   Max Features=5000, Ngram Range=(1, 1)           0.996410   \n9   Max Features=5000, Ngram Range=(2, 2)           0.985192   \n10  Max Features=5000, Ngram Range=(1, 1)           0.995513   \n11  Max Features=5000, Ngram Range=(2, 2)           0.984519   \n12  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n13  Max Features=5000, Ngram Range=(2, 2)           0.995961   \n14  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n15  Max Features=5000, Ngram Range=(2, 2)           0.995064   \n16  Max Features=5000, Ngram Range=(1, 1)           0.981153   \n17  Max Features=5000, Ngram Range=(2, 2)           0.974647   \n18  Max Features=5000, Ngram Range=(1, 1)           0.982499   \n19  Max Features=5000, Ngram Range=(2, 2)           0.973749   \n20  Max Features=5000, Ngram Range=(1, 1)           0.973749   \n21  Max Features=5000, Ngram Range=(2, 2)           0.924389   \n22  Max Features=5000, Ngram Range=(1, 1)           0.973300   \n23  Max Features=5000, Ngram Range=(2, 2)           0.923491   \n24  Max Features=5000, Ngram Range=(1, 1)           0.997756   \n25  Max Features=5000, Ngram Range=(2, 2)           0.995288   \n26  Max Features=5000, Ngram Range=(1, 1)           0.997532   \n27  Max Features=5000, Ngram Range=(2, 2)           0.993942   \n28  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n29  Max Features=5000, Ngram Range=(2, 2)           0.996186   \n30  Max Features=5000, Ngram Range=(1, 1)           1.000000   \n31  Max Features=5000, Ngram Range=(2, 2)           0.995064   \n\n    Testing Accuracy  F1 Micro  F1 Macro  F1 Weighted  Recall Micro  \\\n0           0.983857  0.983857  0.964941     0.983764      0.983857   \n1           0.966816  0.966816  0.921793     0.965166      0.966816   \n2           0.982063  0.982063  0.961046     0.981960      0.982063   \n3           0.967713  0.967713  0.924150     0.966166      0.967713   \n4           0.977578  0.977578  0.948152     0.976703      0.977578   \n5           0.943498  0.943498  0.851283     0.936881      0.943498   \n6           0.977578  0.977578  0.948152     0.976703      0.977578   \n7           0.944395  0.944395  0.854186     0.938017      0.944395   \n8           0.980269  0.980269  0.954796     0.979600      0.980269   \n9           0.949776  0.949776  0.871147     0.944720      0.949776   \n10          0.978475  0.978475  0.950687     0.977745      0.978475   \n11          0.950673  0.950673  0.873901     0.945819      0.950673   \n12          0.980269  0.980269  0.954796     0.979600      0.980269   \n13          0.965919  0.965919  0.918897     0.964036      0.965919   \n14          0.975785  0.975785  0.943651     0.974754      0.975785   \n15          0.966816  0.966816  0.921793     0.965166      0.966816   \n16          0.965919  0.965919  0.917826     0.963777      0.965919   \n17          0.954260  0.954260  0.885521     0.950362      0.954260   \n18          0.965919  0.965919  0.917826     0.963777      0.965919   \n19          0.955157  0.955157  0.888155     0.951430      0.955157   \n20          0.957848  0.957848  0.898027     0.955116      0.957848   \n21          0.910314  0.910314  0.727820     0.890725      0.910314   \n22          0.956951  0.956951  0.895510     0.954076      0.956951   \n23          0.913901  0.913901  0.743275     0.896257      0.913901   \n24          0.981166  0.981166  0.957246     0.980621      0.981166   \n25          0.960538  0.960538  0.903572     0.957746      0.960538   \n26          0.978475  0.978475  0.950989     0.977817      0.978475   \n27          0.959641  0.959641  0.901047     0.956705      0.959641   \n28          0.977578  0.977578  0.948152     0.976703      0.977578   \n29          0.967713  0.967713  0.923662     0.966049      0.967713   \n30          0.976682  0.976682  0.945909     0.975730      0.976682   \n31          0.965022  0.965022  0.917033     0.963155      0.965022   \n\n    Recall Macro  Recall Weighted  Precision Micro  Precision Macro  \\\n0       0.959706         0.983857         0.983857         0.970351   \n1       0.885112         0.966816         0.966816         0.970327   \n2       0.955855         0.982063         0.982063         0.966411   \n3       0.888446         0.967713         0.967713         0.970917   \n4       0.916667         0.977578         0.977578         0.987374   \n5       0.790000         0.943498         0.943498         0.969358   \n6       0.916667         0.977578         0.977578         0.987374   \n7       0.793333         0.944395         0.944395         0.969815   \n8       0.926667         0.980269         0.980269         0.988855   \n9       0.813333         0.949776         0.949776         0.972576   \n10      0.922815         0.978475         0.978475         0.984442   \n11      0.816667         0.950673         0.950673         0.973039   \n12      0.926667         0.980269         0.980269         0.988855   \n13      0.878964         0.965919         0.965919         0.973361   \n14      0.910000         0.975785         0.975785         0.986391   \n15      0.885112         0.966816         0.966816         0.970327   \n16      0.873333         0.965919         0.965919         0.981057   \n17      0.832815         0.954260         0.954260         0.970395   \n18      0.873333         0.965919         0.965919         0.981057   \n19      0.836149         0.955157         0.955157         0.970912   \n20      0.854594         0.957848         0.957848         0.960568   \n21      0.669482         0.910314         0.910314         0.943818   \n22      0.851261         0.956951         0.956951         0.959928   \n23      0.682815         0.913901         0.913901         0.946218   \n24      0.932815         0.981166         0.981166         0.986021   \n25      0.856149         0.960538         0.960538         0.974020   \n26      0.925630         0.978475         0.978475         0.981140   \n27      0.852815         0.959641         0.959641         0.973502   \n28      0.916667         0.977578         0.977578         0.987374   \n29      0.885630         0.967713         0.967713         0.974474   \n30      0.913333         0.976682         0.976682         0.986882   \n31      0.878446         0.965022         0.965022         0.969143   \n\n    Precision Weighted       Confusion Matrix  \n0             0.983725  [[958, 7], [11, 139]]  \n1             0.967064  [[962, 3], [34, 116]]  \n2             0.981911  [[957, 8], [12, 138]]  \n3             0.967933  [[962, 3], [33, 117]]  \n4             0.978145  [[965, 0], [25, 125]]  \n5             0.946960   [[965, 0], [63, 87]]  \n6             0.978145  [[965, 0], [25, 125]]  \n7             0.947752   [[965, 0], [62, 88]]  \n8             0.980709  [[965, 0], [22, 128]]  \n9             0.952530   [[965, 0], [56, 94]]  \n10            0.978781  [[964, 1], [23, 127]]  \n11            0.953332   [[965, 0], [55, 95]]  \n12            0.980709  [[965, 0], [22, 128]]  \n13            0.966492  [[963, 2], [36, 114]]  \n14            0.976444  [[965, 0], [27, 123]]  \n15            0.967064  [[962, 3], [34, 116]]  \n16            0.967210  [[965, 0], [38, 112]]  \n17            0.955992  [[964, 1], [50, 100]]  \n18            0.967210  [[965, 0], [38, 112]]  \n19            0.956817  [[964, 1], [49, 101]]  \n20            0.958085  [[961, 4], [43, 107]]  \n21            0.916809   [[964, 1], [99, 51]]  \n22            0.957217  [[961, 4], [44, 106]]  \n23            0.919959   [[964, 1], [95, 55]]  \n24            0.981382  [[964, 1], [20, 130]]  \n25            0.961798  [[964, 1], [43, 107]]  \n26            0.978600  [[963, 2], [22, 128]]  \n27            0.960964  [[964, 1], [44, 106]]  \n28            0.978145  [[965, 0], [25, 125]]  \n29            0.968205  [[963, 2], [34, 116]]  \n30            0.977293  [[965, 0], [26, 124]]  \n31            0.965331  [[962, 3], [36, 114]]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Experiment</th>\n      <th>Vectorizer</th>\n      <th>Model</th>\n      <th>Preprocessing</th>\n      <th>Parameters</th>\n      <th>Training Accuracy</th>\n      <th>Testing Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>F1 Weighted</th>\n      <th>Recall Micro</th>\n      <th>Recall Macro</th>\n      <th>Recall Weighted</th>\n      <th>Precision Micro</th>\n      <th>Precision Macro</th>\n      <th>Precision Weighted</th>\n      <th>Confusion Matrix</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CountVectorizer_MultinomialNB_stemmingTrue_lem...</td>\n      <td>CountVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.992147</td>\n      <td>0.983857</td>\n      <td>0.983857</td>\n      <td>0.964941</td>\n      <td>0.983764</td>\n      <td>0.983857</td>\n      <td>0.959706</td>\n      <td>0.983857</td>\n      <td>0.983857</td>\n      <td>0.970351</td>\n      <td>0.983725</td>\n      <td>[[958, 7], [11, 139]]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CountVectorizer_MultinomialNB_stemmingTrue_lem...</td>\n      <td>CountVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.987660</td>\n      <td>0.966816</td>\n      <td>0.966816</td>\n      <td>0.921793</td>\n      <td>0.965166</td>\n      <td>0.966816</td>\n      <td>0.885112</td>\n      <td>0.966816</td>\n      <td>0.966816</td>\n      <td>0.970327</td>\n      <td>0.967064</td>\n      <td>[[962, 3], [34, 116]]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CountVectorizer_MultinomialNB_stemmingFalse_le...</td>\n      <td>CountVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.990577</td>\n      <td>0.982063</td>\n      <td>0.982063</td>\n      <td>0.961046</td>\n      <td>0.981960</td>\n      <td>0.982063</td>\n      <td>0.955855</td>\n      <td>0.982063</td>\n      <td>0.982063</td>\n      <td>0.966411</td>\n      <td>0.981911</td>\n      <td>[[957, 8], [12, 138]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CountVectorizer_MultinomialNB_stemmingFalse_le...</td>\n      <td>CountVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.986089</td>\n      <td>0.967713</td>\n      <td>0.967713</td>\n      <td>0.924150</td>\n      <td>0.966166</td>\n      <td>0.967713</td>\n      <td>0.888446</td>\n      <td>0.967713</td>\n      <td>0.967713</td>\n      <td>0.970917</td>\n      <td>0.967933</td>\n      <td>[[962, 3], [33, 117]]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CountVectorizer_LogisticRegression_stemmingTru...</td>\n      <td>CountVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.996186</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.948152</td>\n      <td>0.976703</td>\n      <td>0.977578</td>\n      <td>0.916667</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.987374</td>\n      <td>0.978145</td>\n      <td>[[965, 0], [25, 125]]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CountVectorizer_LogisticRegression_stemmingTru...</td>\n      <td>CountVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.973525</td>\n      <td>0.943498</td>\n      <td>0.943498</td>\n      <td>0.851283</td>\n      <td>0.936881</td>\n      <td>0.943498</td>\n      <td>0.790000</td>\n      <td>0.943498</td>\n      <td>0.943498</td>\n      <td>0.969358</td>\n      <td>0.946960</td>\n      <td>[[965, 0], [63, 87]]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CountVectorizer_LogisticRegression_stemmingFal...</td>\n      <td>CountVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.995737</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.948152</td>\n      <td>0.976703</td>\n      <td>0.977578</td>\n      <td>0.916667</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.987374</td>\n      <td>0.978145</td>\n      <td>[[965, 0], [25, 125]]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CountVectorizer_LogisticRegression_stemmingFal...</td>\n      <td>CountVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.973076</td>\n      <td>0.944395</td>\n      <td>0.944395</td>\n      <td>0.854186</td>\n      <td>0.938017</td>\n      <td>0.944395</td>\n      <td>0.793333</td>\n      <td>0.944395</td>\n      <td>0.944395</td>\n      <td>0.969815</td>\n      <td>0.947752</td>\n      <td>[[965, 0], [62, 88]]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CountVectorizer_SVM_stemmingTrue_lemmatization...</td>\n      <td>CountVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.996410</td>\n      <td>0.980269</td>\n      <td>0.980269</td>\n      <td>0.954796</td>\n      <td>0.979600</td>\n      <td>0.980269</td>\n      <td>0.926667</td>\n      <td>0.980269</td>\n      <td>0.980269</td>\n      <td>0.988855</td>\n      <td>0.980709</td>\n      <td>[[965, 0], [22, 128]]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CountVectorizer_SVM_stemmingTrue_lemmatization...</td>\n      <td>CountVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.985192</td>\n      <td>0.949776</td>\n      <td>0.949776</td>\n      <td>0.871147</td>\n      <td>0.944720</td>\n      <td>0.949776</td>\n      <td>0.813333</td>\n      <td>0.949776</td>\n      <td>0.949776</td>\n      <td>0.972576</td>\n      <td>0.952530</td>\n      <td>[[965, 0], [56, 94]]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CountVectorizer_SVM_stemmingFalse_lemmatizatio...</td>\n      <td>CountVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.995513</td>\n      <td>0.978475</td>\n      <td>0.978475</td>\n      <td>0.950687</td>\n      <td>0.977745</td>\n      <td>0.978475</td>\n      <td>0.922815</td>\n      <td>0.978475</td>\n      <td>0.978475</td>\n      <td>0.984442</td>\n      <td>0.978781</td>\n      <td>[[964, 1], [23, 127]]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CountVectorizer_SVM_stemmingFalse_lemmatizatio...</td>\n      <td>CountVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.984519</td>\n      <td>0.950673</td>\n      <td>0.950673</td>\n      <td>0.873901</td>\n      <td>0.945819</td>\n      <td>0.950673</td>\n      <td>0.816667</td>\n      <td>0.950673</td>\n      <td>0.950673</td>\n      <td>0.973039</td>\n      <td>0.953332</td>\n      <td>[[965, 0], [55, 95]]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>CountVectorizer_RandomForest_stemmingTrue_lemm...</td>\n      <td>CountVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>1.000000</td>\n      <td>0.980269</td>\n      <td>0.980269</td>\n      <td>0.954796</td>\n      <td>0.979600</td>\n      <td>0.980269</td>\n      <td>0.926667</td>\n      <td>0.980269</td>\n      <td>0.980269</td>\n      <td>0.988855</td>\n      <td>0.980709</td>\n      <td>[[965, 0], [22, 128]]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>CountVectorizer_RandomForest_stemmingTrue_lemm...</td>\n      <td>CountVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.995961</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.918897</td>\n      <td>0.964036</td>\n      <td>0.965919</td>\n      <td>0.878964</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.973361</td>\n      <td>0.966492</td>\n      <td>[[963, 2], [36, 114]]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>CountVectorizer_RandomForest_stemmingFalse_lem...</td>\n      <td>CountVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>1.000000</td>\n      <td>0.975785</td>\n      <td>0.975785</td>\n      <td>0.943651</td>\n      <td>0.974754</td>\n      <td>0.975785</td>\n      <td>0.910000</td>\n      <td>0.975785</td>\n      <td>0.975785</td>\n      <td>0.986391</td>\n      <td>0.976444</td>\n      <td>[[965, 0], [27, 123]]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>CountVectorizer_RandomForest_stemmingFalse_lem...</td>\n      <td>CountVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.995064</td>\n      <td>0.966816</td>\n      <td>0.966816</td>\n      <td>0.921793</td>\n      <td>0.965166</td>\n      <td>0.966816</td>\n      <td>0.885112</td>\n      <td>0.966816</td>\n      <td>0.966816</td>\n      <td>0.970327</td>\n      <td>0.967064</td>\n      <td>[[962, 3], [34, 116]]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>TfidfVectorizer_MultinomialNB_stemmingTrue_lem...</td>\n      <td>TfidfVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.981153</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.917826</td>\n      <td>0.963777</td>\n      <td>0.965919</td>\n      <td>0.873333</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.981057</td>\n      <td>0.967210</td>\n      <td>[[965, 0], [38, 112]]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>TfidfVectorizer_MultinomialNB_stemmingTrue_lem...</td>\n      <td>TfidfVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.974647</td>\n      <td>0.954260</td>\n      <td>0.954260</td>\n      <td>0.885521</td>\n      <td>0.950362</td>\n      <td>0.954260</td>\n      <td>0.832815</td>\n      <td>0.954260</td>\n      <td>0.954260</td>\n      <td>0.970395</td>\n      <td>0.955992</td>\n      <td>[[964, 1], [50, 100]]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>TfidfVectorizer_MultinomialNB_stemmingFalse_le...</td>\n      <td>TfidfVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.982499</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.917826</td>\n      <td>0.963777</td>\n      <td>0.965919</td>\n      <td>0.873333</td>\n      <td>0.965919</td>\n      <td>0.965919</td>\n      <td>0.981057</td>\n      <td>0.967210</td>\n      <td>[[965, 0], [38, 112]]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>TfidfVectorizer_MultinomialNB_stemmingFalse_le...</td>\n      <td>TfidfVectorizer</td>\n      <td>MultinomialNB</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.973749</td>\n      <td>0.955157</td>\n      <td>0.955157</td>\n      <td>0.888155</td>\n      <td>0.951430</td>\n      <td>0.955157</td>\n      <td>0.836149</td>\n      <td>0.955157</td>\n      <td>0.955157</td>\n      <td>0.970912</td>\n      <td>0.956817</td>\n      <td>[[964, 1], [49, 101]]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>TfidfVectorizer_LogisticRegression_stemmingTru...</td>\n      <td>TfidfVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.973749</td>\n      <td>0.957848</td>\n      <td>0.957848</td>\n      <td>0.898027</td>\n      <td>0.955116</td>\n      <td>0.957848</td>\n      <td>0.854594</td>\n      <td>0.957848</td>\n      <td>0.957848</td>\n      <td>0.960568</td>\n      <td>0.958085</td>\n      <td>[[961, 4], [43, 107]]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>TfidfVectorizer_LogisticRegression_stemmingTru...</td>\n      <td>TfidfVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.924389</td>\n      <td>0.910314</td>\n      <td>0.910314</td>\n      <td>0.727820</td>\n      <td>0.890725</td>\n      <td>0.910314</td>\n      <td>0.669482</td>\n      <td>0.910314</td>\n      <td>0.910314</td>\n      <td>0.943818</td>\n      <td>0.916809</td>\n      <td>[[964, 1], [99, 51]]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>TfidfVectorizer_LogisticRegression_stemmingFal...</td>\n      <td>TfidfVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.973300</td>\n      <td>0.956951</td>\n      <td>0.956951</td>\n      <td>0.895510</td>\n      <td>0.954076</td>\n      <td>0.956951</td>\n      <td>0.851261</td>\n      <td>0.956951</td>\n      <td>0.956951</td>\n      <td>0.959928</td>\n      <td>0.957217</td>\n      <td>[[961, 4], [44, 106]]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>TfidfVectorizer_LogisticRegression_stemmingFal...</td>\n      <td>TfidfVectorizer</td>\n      <td>LogisticRegression</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.923491</td>\n      <td>0.913901</td>\n      <td>0.913901</td>\n      <td>0.743275</td>\n      <td>0.896257</td>\n      <td>0.913901</td>\n      <td>0.682815</td>\n      <td>0.913901</td>\n      <td>0.913901</td>\n      <td>0.946218</td>\n      <td>0.919959</td>\n      <td>[[964, 1], [95, 55]]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>TfidfVectorizer_SVM_stemmingTrue_lemmatization...</td>\n      <td>TfidfVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.997756</td>\n      <td>0.981166</td>\n      <td>0.981166</td>\n      <td>0.957246</td>\n      <td>0.980621</td>\n      <td>0.981166</td>\n      <td>0.932815</td>\n      <td>0.981166</td>\n      <td>0.981166</td>\n      <td>0.986021</td>\n      <td>0.981382</td>\n      <td>[[964, 1], [20, 130]]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>TfidfVectorizer_SVM_stemmingTrue_lemmatization...</td>\n      <td>TfidfVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.995288</td>\n      <td>0.960538</td>\n      <td>0.960538</td>\n      <td>0.903572</td>\n      <td>0.957746</td>\n      <td>0.960538</td>\n      <td>0.856149</td>\n      <td>0.960538</td>\n      <td>0.960538</td>\n      <td>0.974020</td>\n      <td>0.961798</td>\n      <td>[[964, 1], [43, 107]]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...</td>\n      <td>TfidfVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>0.997532</td>\n      <td>0.978475</td>\n      <td>0.978475</td>\n      <td>0.950989</td>\n      <td>0.977817</td>\n      <td>0.978475</td>\n      <td>0.925630</td>\n      <td>0.978475</td>\n      <td>0.978475</td>\n      <td>0.981140</td>\n      <td>0.978600</td>\n      <td>[[963, 2], [22, 128]]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>TfidfVectorizer_SVM_stemmingFalse_lemmatizatio...</td>\n      <td>TfidfVectorizer</td>\n      <td>SVM</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.993942</td>\n      <td>0.959641</td>\n      <td>0.959641</td>\n      <td>0.901047</td>\n      <td>0.956705</td>\n      <td>0.959641</td>\n      <td>0.852815</td>\n      <td>0.959641</td>\n      <td>0.959641</td>\n      <td>0.973502</td>\n      <td>0.960964</td>\n      <td>[[964, 1], [44, 106]]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>TfidfVectorizer_RandomForest_stemmingTrue_lemm...</td>\n      <td>TfidfVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>1.000000</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.948152</td>\n      <td>0.976703</td>\n      <td>0.977578</td>\n      <td>0.916667</td>\n      <td>0.977578</td>\n      <td>0.977578</td>\n      <td>0.987374</td>\n      <td>0.978145</td>\n      <td>[[965, 0], [25, 125]]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>TfidfVectorizer_RandomForest_stemmingTrue_lemm...</td>\n      <td>TfidfVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=True, Lemmatization=False</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.996186</td>\n      <td>0.967713</td>\n      <td>0.967713</td>\n      <td>0.923662</td>\n      <td>0.966049</td>\n      <td>0.967713</td>\n      <td>0.885630</td>\n      <td>0.967713</td>\n      <td>0.967713</td>\n      <td>0.974474</td>\n      <td>0.968205</td>\n      <td>[[963, 2], [34, 116]]</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>TfidfVectorizer_RandomForest_stemmingFalse_lem...</td>\n      <td>TfidfVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(1, 1)</td>\n      <td>1.000000</td>\n      <td>0.976682</td>\n      <td>0.976682</td>\n      <td>0.945909</td>\n      <td>0.975730</td>\n      <td>0.976682</td>\n      <td>0.913333</td>\n      <td>0.976682</td>\n      <td>0.976682</td>\n      <td>0.986882</td>\n      <td>0.977293</td>\n      <td>[[965, 0], [26, 124]]</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>TfidfVectorizer_RandomForest_stemmingFalse_lem...</td>\n      <td>TfidfVectorizer</td>\n      <td>RandomForest</td>\n      <td>Stemming=False, Lemmatization=True</td>\n      <td>Max Features=5000, Ngram Range=(2, 2)</td>\n      <td>0.995064</td>\n      <td>0.965022</td>\n      <td>0.965022</td>\n      <td>0.917033</td>\n      <td>0.963155</td>\n      <td>0.965022</td>\n      <td>0.878446</td>\n      <td>0.965022</td>\n      <td>0.965022</td>\n      <td>0.969143</td>\n      <td>0.965331</td>\n      <td>[[962, 3], [36, 114]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## The Metric is depend on the Domain Expertise and Problem statement , Here i take the mean of all 3 F1 Scores because it has both Precision and Recall","metadata":{}},{"cell_type":"code","source":"# Calculate the mean F1 scores for each experiment\nresults_df[\"Mean F1\"] = results_df[[\"F1 Micro\", \"F1 Macro\", \"F1 Weighted\"]].mean(axis=1)\n\n# Find the experiment with the highest mean F1 score\nbest_experiment = results_df.loc[results_df[\"Mean F1\"].idxmax()]\n\n# Print the best experiment details\nprint(\"Best Experiment:\")\nprint(best_experiment)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T06:52:43.919177Z","iopub.execute_input":"2023-09-05T06:52:43.919969Z","iopub.status.idle":"2023-09-05T06:52:43.933208Z","shell.execute_reply.started":"2023-09-05T06:52:43.919929Z","shell.execute_reply":"2023-09-05T06:52:43.931679Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Best Experiment:\nExperiment            CountVectorizer_MultinomialNB_stemmingTrue_lem...\nVectorizer                                              CountVectorizer\nModel                                                     MultinomialNB\nPreprocessing                        Stemming=True, Lemmatization=False\nParameters                        Max Features=5000, Ngram Range=(1, 1)\nTraining Accuracy                                              0.992147\nTesting Accuracy                                               0.983857\nF1 Micro                                                       0.983857\nF1 Macro                                                       0.964941\nF1 Weighted                                                    0.983764\nRecall Micro                                                   0.983857\nRecall Macro                                                   0.959706\nRecall Weighted                                                0.983857\nPrecision Micro                                                0.983857\nPrecision Macro                                                0.970351\nPrecision Weighted                                             0.983725\nConfusion Matrix                                  [[958, 7], [11, 139]]\nMean F1                                                        0.977521\nName: 0, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## We are getting Good results for this Experiment","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}